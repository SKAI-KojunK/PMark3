# PMark3 Production ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” PMark3 í”„ë¡œí† íƒ€ì…ì„ Production í™˜ê²½ìœ¼ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ê¸° ìœ„í•œ ì¢…í•©ì ì¸ ê°€ì´ë“œì…ë‹ˆë‹¤. LangGraph Agent ì „í™˜, Azure ì¸í”„ë¼ êµ¬ì¶•, ë¡œì»¬ LLM ì„œë¹™, ë²¡í„° DB êµ¬í˜„, ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ ë“±ì„ ë‹¤ë£¹ë‹ˆë‹¤.

## ğŸ¯ Production ì „í™˜ ëª©í‘œ

### 1. LangGraph Agent ê¸°ë°˜ ì•„í‚¤í…ì²˜
- í˜„ì¬ ê°œë³„ í•¨ìˆ˜ë“¤ì„ LangGraphì˜ ë…¸ë“œì™€ ì—£ì§€ë¡œ ì¬êµ¬ì„±
- ìƒíƒœ ê´€ë¦¬ì™€ ì›Œí¬í”Œë¡œìš° ìµœì í™”
- ë³µì¡í•œ ì‘ì—… íë¦„ì˜ ì‹œê°í™” ë° ê´€ë¦¬

### 2. Azure ì¸í”„ë¼ êµ¬ì„±
- Azure SQL Databaseì™€ ì—°ë™
- Azure Container Instances ë˜ëŠ” AKS ë°°í¬
- Azure Cognitive Services í†µí•©

### 3. ë¡œì»¬ LLM ì„œë¹™
- vLLM ê¸°ë°˜ Mistral 7B Instruct ë˜ëŠ” Qwen3 14B ì„œë¹™
- GPU ìµœì í™” ë° ë°°ì¹˜ ì²˜ë¦¬
- API ê²Œì´íŠ¸ì›¨ì´ êµ¬ì„±

### 4. ë²¡í„° ì„ë² ë”© ì‹œìŠ¤í…œ
- ì„¤ë¹„ìœ í˜•, ìœ„ì¹˜, ì‘ì—…ìƒì„¸ ë²¡í„° DB êµ¬ì¶•
- ì‹¤ì‹œê°„ ì„ë² ë”© ìƒì„± ë° ê²€ìƒ‰
- ì •ê·œí™” ì •í™•ë„ í–¥ìƒ

### 5. ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
- ìë™í™”ëœ í’ˆì§ˆ í‰ê°€

---

## ğŸ—ï¸ LangGraph Agent ì•„í‚¤í…ì²˜ ì„¤ê³„

### 1. í˜„ì¬ êµ¬ì¡° ë¶„ì„

```python
# í˜„ì¬ í”„ë¡œí† íƒ€ì… êµ¬ì¡° (ìˆœì°¨ì )
user_input â†’ InputParser â†’ Normalizer â†’ Recommender â†’ ResponseGenerator â†’ response

# LangGraph êµ¬ì¡° (ê·¸ë˜í”„ ê¸°ë°˜)
user_input â†’ [StateManager] â†’ [ParsingNode] â†’ [NormalizationNode] â†’ [RecommendationNode] â†’ [ResponseNode] â†’ response
              â†‘                    â†“                â†“                     â†“                   â†“
              â””â”€â”€â”€ [SessionNode] â†â”€â”€â”´â”€â”€â”€â”€ [VectorSearchNode] â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€ [EvaluationNode]
```

### 2. LangGraph ë…¸ë“œ ì„¤ê³„

#### StateManager (ìƒíƒœ ê´€ë¦¬ì)
```python
# ëª©ì : ì „ì²´ ì›Œí¬í”Œë¡œìš°ì˜ ìƒíƒœ ê´€ë¦¬
# í˜„ì¬ ëŒ€ì‘: session_manager.py
# Production ê°œì„ ì‚¬í•­:
# - ë¶„ì‚° ìƒíƒœ ê´€ë¦¬ (Redis/Azure Cache)
# - ìƒíƒœ ì˜ì†ì„± ë³´ì¥
# - ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„

class WorkflowState(TypedDict):
    user_input: str
    session_id: str
    parsed_data: Optional[ParsedInput]
    normalized_data: Optional[Dict]
    recommendations: Optional[List[Recommendation]]
    response: Optional[str]
    context: Dict[str, Any]
    metrics: Dict[str, float]
```

#### ParsingNode (íŒŒì‹± ë…¸ë“œ)
```python
# ëª©ì : ì‚¬ìš©ì ì…ë ¥ íŒŒì‹± ë° ì •ë³´ ì¶”ì¶œ
# í˜„ì¬ ëŒ€ì‘: agents/parser.py
# Production ê°œì„ ì‚¬í•­:
# - ë¡œì»¬ LLM ì—°ë™
# - ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
# - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

def parsing_node(state: WorkflowState) -> WorkflowState:
    """
    íŒŒì‹± ë…¸ë“œ - ì‚¬ìš©ì ì…ë ¥ì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜
    
    ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
    - vLLM ì—”ë“œí¬ì¸íŠ¸ì™€ ì—°ë™
    - ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ê¸° ë¡œì§ êµ¬í˜„
    - ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
    """
    # ì„±ëŠ¥ ì¸¡ì • ì‹œì‘
    start_time = time.time()
    
    # íŒŒì‹± ë¡œì§ ì‹¤í–‰
    parser = ProductionInputParser(llm_endpoint=state["config"]["vllm_endpoint"])
    parsed_data = parser.parse_with_context(
        user_input=state["user_input"],
        session_context=state["context"]
    )
    
    # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    state["metrics"]["parsing_time"] = time.time() - start_time
    state["metrics"]["parsing_confidence"] = parsed_data.confidence
    
    state["parsed_data"] = parsed_data
    return state
```

### 3. ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ì •ì˜

```python
# LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
from langgraph.graph import StateGraph, END

def create_production_workflow():
    """
    Productionìš© LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±
    
    AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
    - ê° ë…¸ë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
    - ì¡°ê±´ë¶€ ë¶„ê¸°ë¡œ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬
    - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸ í¬í•¨
    """
    workflow = StateGraph(WorkflowState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("parse", parsing_node)
    workflow.add_node("normalize", normalization_node)
    workflow.add_node("search_vector", vector_search_node)
    workflow.add_node("recommend", recommendation_node)
    workflow.add_node("generate_response", response_generation_node)
    workflow.add_node("evaluate", evaluation_node)
    
    # ì—£ì§€ ì¶”ê°€ (ì¡°ê±´ë¶€ ë¶„ê¸° í¬í•¨)
    workflow.add_edge("parse", "normalize")
    workflow.add_conditional_edges(
        "normalize",
        should_use_vector_search,
        {
            "vector": "search_vector",
            "traditional": "recommend"
        }
    )
    workflow.add_edge("search_vector", "recommend")
    workflow.add_edge("recommend", "generate_response")
    workflow.add_edge("generate_response", "evaluate")
    workflow.add_edge("evaluate", END)
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("parse")
    
    return workflow.compile()
```

---

## ğŸ”§ Azure ì¸í”„ë¼ ì—°ë™ ì„¤ê³„

### 1. ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ (SQLite â†’ Azure SQL)

#### í˜„ì¬ êµ¬ì¡° ë¶„ì„
```python
# í˜„ì¬: backend/app/database.py
class DatabaseManager:
    def __init__(self):
        self.conn = sqlite3.connect("./data/notifications.db")
    
    def search_similar_notifications(self, ...):
        # SQLite ì „ìš© ì¿¼ë¦¬
        query = "SELECT ... FROM notification_history WHERE ..."
```

#### Production êµ¬ì¡° ì„¤ê³„
```python
# Production: ì¶”ìƒí™” ê³„ì¸µ ë„ì…
from abc import ABC, abstractmethod
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

class DatabaseInterface(ABC):
    """
    ë°ì´í„°ë² ì´ìŠ¤ ì¸í„°í˜ì´ìŠ¤ - ë‹¤ì–‘í•œ DB ì§€ì›
    
    ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
    - SQLite, Azure SQL, PostgreSQL ì§€ì›
    - ì—°ê²° í’€ë§ ë° íŠ¸ëœì­ì…˜ ê´€ë¦¬
    - ìë™ ì¬ì—°ê²° ë° ì¥ì•  ë³µêµ¬
    """
    
    @abstractmethod
    def search_similar_notifications(self, filters: Dict) -> List[Dict]:
        pass
    
    @abstractmethod
    def get_standard_terms(self, category: str) -> List[str]:
        pass

class AzureSQLManager(DatabaseInterface):
    """
    Azure SQL Database ë§¤ë‹ˆì €
    
    AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
    - ê¸°ì¡´ SQLite ì¿¼ë¦¬ë¥¼ T-SQLë¡œ ìë™ ë³€í™˜
    - ì¸ë±ìŠ¤ íŒíŠ¸ ë° ì¿¼ë¦¬ ìµœì í™”
    - ì—°ê²° ìƒíƒœ ëª¨ë‹ˆí„°ë§
    """
    
    def __init__(self, connection_string: str):
        self.engine = create_engine(
            connection_string,
            pool_size=20,
            max_overflow=0,
            pool_pre_ping=True,
            pool_recycle=3600
        )
        self.Session = sessionmaker(bind=self.engine)
    
    def search_similar_notifications(self, filters: Dict) -> List[Dict]:
        """
        Azure SQL ìµœì í™”ëœ ìœ ì‚¬ ì•Œë¦¼ ê²€ìƒ‰
        
        ê°œì„ ì‚¬í•­:
        - ì¸ë±ìŠ¤ ê¸°ë°˜ ê²€ìƒ‰ ìµœì í™”
        - ë³‘ë ¬ ì¿¼ë¦¬ ì‹¤í–‰
        - ìºì‹œ ë ˆì´ì–´ í†µí•©
        """
        with self.Session() as session:
            # T-SQL ìµœì í™” ì¿¼ë¦¬
            query = text("""
                SELECT 
                    itemno, process, location, equipType, statusCode, 
                    work_title, work_details, priority,
                    -- ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°ì„ SQL ë ˆë²¨ì—ì„œ ìˆ˜í–‰
                    (
                        CASE WHEN location LIKE :location THEN 35 ELSE 0 END +
                        CASE WHEN equipType LIKE :equipment THEN 35 ELSE 0 END +
                        CASE WHEN statusCode LIKE :status THEN 20 ELSE 0 END +
                        CASE WHEN priority LIKE :priority THEN 10 ELSE 0 END
                    ) / 100.0 as similarity_score
                FROM notification_history 
                WHERE 
                    (:location IS NULL OR location LIKE :location OR process LIKE :location)
                    AND (:equipment IS NULL OR equipType LIKE :equipment)
                    AND (:status IS NULL OR statusCode LIKE :status)
                    AND (:priority IS NULL OR priority LIKE :priority)
                ORDER BY similarity_score DESC, created_at DESC
                LIMIT :limit
            """)
            
            result = session.execute(query, filters)
            return [dict(row) for row in result.fetchall()]
```

### 2. Azure ì„œë¹„ìŠ¤ í†µí•©

```python
# Azure ì„œë¹„ìŠ¤ ì—°ë™ í´ë˜ìŠ¤
class AzureServiceIntegration:
    """
    Azure í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ í†µí•© ê´€ë¦¬ì
    
    ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
    - Azure Key Vaultë¥¼ í†µí•œ ì‹œí¬ë¦¿ ê´€ë¦¬
    - Azure Monitorë¥¼ í†µí•œ ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
    - Azure Service Busë¥¼ í†µí•œ ë©”ì‹œì§€ íì‰
    """
    
    def __init__(self, config: AzureConfig):
        self.key_vault_client = SecretClient(
            vault_url=config.key_vault_url,
            credential=DefaultAzureCredential()
        )
        self.monitor_client = MetricsQueryClient(DefaultAzureCredential())
        self.servicebus_client = ServiceBusClient.from_connection_string(
            config.servicebus_connection_string
        )
    
    async def get_secret(self, secret_name: str) -> str:
        """Azure Key Vaultì—ì„œ ì‹œí¬ë¦¿ ì¡°íšŒ"""
        try:
            secret = await self.key_vault_client.get_secret(secret_name)
            return secret.value
        except Exception as e:
            logger.error(f"Failed to retrieve secret {secret_name}: {e}")
            raise
    
    async def send_metrics(self, metrics: Dict[str, float]):
        """Azure Monitorë¡œ ë©”íŠ¸ë¦­ ì „ì†¡"""
        # ì„±ëŠ¥ ì§€í‘œë¥¼ Azure Monitorë¡œ ì „ì†¡
        pass
```

---

## ğŸ¤– ë¡œì»¬ LLM ì„œë¹™ ì•„í‚¤í…ì²˜

### 1. vLLM ì„œë²„ êµ¬ì„±

```python
# vLLM ì„œë²„ ì„¤ì • í´ë˜ìŠ¤
class vLLMServerConfig:
    """
    vLLM ì„œë²„ ì„¤ì • ê´€ë¦¬
    
    AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
    - GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
    - ë°°ì¹˜ í¬ê¸° ë™ì  ì¡°ì ˆ
    - ëª¨ë¸ë³„ ìµœì  íŒŒë¼ë¯¸í„° ê´€ë¦¬
    """
    
    def __init__(self):
        self.models = {
            "mistral-7b": {
                "model_path": "/models/mistral-7b-instruct",
                "tensor_parallel_size": 1,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.9,
                "enforce_eager": False
            },
            "qwen3-14b": {
                "model_path": "/models/qwen3-14b-instruct", 
                "tensor_parallel_size": 2,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.85,
                "enforce_eager": True
            }
        }
    
    def get_optimal_config(self, model_name: str, available_gpus: int) -> Dict:
        """
        í•˜ë“œì›¨ì–´ ë¦¬ì†ŒìŠ¤ì— ë”°ë¥¸ ìµœì  ì„¤ì • ë°˜í™˜
        
        ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
        - GPU ìˆ˜ì— ë”°ë¥¸ ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ì¡°ì ˆ
        - ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ
        """
        base_config = self.models.get(model_name, self.models["mistral-7b"])
        
        # GPU ìˆ˜ì— ë”°ë¥¸ ë³‘ë ¬ ì²˜ë¦¬ ì¡°ì ˆ
        if available_gpus >= base_config["tensor_parallel_size"]:
            return base_config
        else:
            # GPUê°€ ë¶€ì¡±í•œ ê²½ìš° ë‹¨ì¼ GPU ì„¤ì •ìœ¼ë¡œ í´ë°±
            return {**base_config, "tensor_parallel_size": 1}

class LocalLLMClient:
    """
    ë¡œì»¬ LLM í´ë¼ì´ì–¸íŠ¸ - vLLM ì„œë²„ì™€ í†µì‹ 
    
    í˜„ì¬ ëŒ€ì‘: OpenAI í´ë¼ì´ì–¸íŠ¸ í˜¸ì¶œ ë¶€ë¶„
    Production ê°œì„ ì‚¬í•­:
    - ì—°ê²° í’€ë§ ë° ì¬ì‚¬ìš©
    - ìë™ ì¬ì—°ê²° ë° ì¥ì•  ì¡°ì¹˜
    - ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
    """
    
    def __init__(self, vllm_endpoint: str, model_name: str):
        self.endpoint = vllm_endpoint
        self.model_name = model_name
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=60),
            connector=aiohttp.TCPConnector(limit=100)
        )
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """
        ë¡œì»¬ LLMì„ í†µí•œ í…ìŠ¤íŠ¸ ìƒì„±
        
        AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
        - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìë™ ì ìš©
        - í† í° ìˆ˜ ì œí•œ ê´€ë¦¬
        - ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
        """
        payload = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": kwargs.get("max_tokens", 512),
            "temperature": kwargs.get("temperature", 0.1),
            "stream": kwargs.get("stream", False)
        }
        
        try:
            async with self.session.post(
                f"{self.endpoint}/v1/chat/completions",
                json=payload
            ) as response:
                result = await response.json()
                return result["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"vLLM request failed: {e}")
            raise
```

### 2. LLM í˜¸ì¶œ ìµœì í™”

```python
class OptimizedLLMService:
    """
    ìµœì í™”ëœ LLM ì„œë¹„ìŠ¤ - ë°°ì¹˜ ì²˜ë¦¬ ë° ìºì‹±
    
    ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
    - ìš”ì²­ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì²˜ë¦¬ëŸ‰ í–¥ìƒ
    - ì‘ë‹µ ìºì‹±ìœ¼ë¡œ ì¤‘ë³µ ìš”ì²­ ìµœì í™”
    - ìš°ì„ ìˆœìœ„ ê¸°ë°˜ í ê´€ë¦¬
    """
    
    def __init__(self, llm_client: LocalLLMClient):
        self.llm_client = llm_client
        self.request_queue = asyncio.Queue()
        self.response_cache = TTLCache(maxsize=1000, ttl=3600)
        self.batch_size = 8
        self.batch_timeout = 0.1
    
    async def process_batch_requests(self):
        """
        ë°°ì¹˜ ìš”ì²­ ì²˜ë¦¬ - ì²˜ë¦¬ëŸ‰ ìµœì í™”
        
        AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
        - ë™ì¼í•œ íƒ€ì…ì˜ ìš”ì²­ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬
        - GPU í™œìš©ë¥  ìµœì í™”
        - ì‘ë‹µ ì‹œê°„ê³¼ ì²˜ë¦¬ëŸ‰ì˜ ê· í˜•
        """
        while True:
            batch = []
            try:
                # ë°°ì¹˜ ìˆ˜ì§‘ (íƒ€ì„ì•„ì›ƒ ë˜ëŠ” ë°°ì¹˜ í¬ê¸° ë„ë‹¬ê¹Œì§€)
                batch.append(await asyncio.wait_for(
                    self.request_queue.get(), 
                    timeout=self.batch_timeout
                ))
                
                # ì¶”ê°€ ìš”ì²­ ìˆ˜ì§‘
                for _ in range(self.batch_size - 1):
                    try:
                        batch.append(await asyncio.wait_for(
                            self.request_queue.get(), 
                            timeout=0.01
                        ))
                    except asyncio.TimeoutError:
                        break
                
                # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
                await self._process_batch(batch)
                
            except asyncio.TimeoutError:
                if batch:
                    await self._process_batch(batch)
            except Exception as e:
                logger.error(f"Batch processing error: {e}")
    
    async def _process_batch(self, batch: List[Tuple]):
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        # ë³‘ë ¬ LLM í˜¸ì¶œ
        tasks = [
            self.llm_client.generate(prompt, **kwargs) 
            for prompt, kwargs, future in batch
        ]
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ë°˜í™˜
        for (prompt, kwargs, future), result in zip(batch, results):
            if isinstance(result, Exception):
                future.set_exception(result)
            else:
                future.set_result(result)
                # ìºì‹œ ì €ì¥
                cache_key = hash(prompt + str(kwargs))
                self.response_cache[cache_key] = result
```

---

## ğŸ” ë²¡í„° ì„ë² ë”© ì‹œìŠ¤í…œ ì„¤ê³„

### 1. í˜„ì¬ êµ¬ì¡°ì™€ ë¹„êµ

#### í˜„ì¬ ì •ê·œí™” ë°©ì‹
```python
# í˜„ì¬: backend/app/logic/normalizer.py
class LLMNormalizer:
    def normalize_term(self, term: str, category: str) -> Tuple[str, float]:
        """
        LLM ê¸°ë°˜ ìš©ì–´ ì •ê·œí™”
        
        í•œê³„ì :
        - LLM í˜¸ì¶œë¡œ ì¸í•œ ì§€ì—° ì‹œê°„
        - ì¼ê´€ì„± ë¶€ì¡± (ê°™ì€ ì…ë ¥ì— ë‹¤ë¥¸ ê²°ê³¼)
        - í‘œì¤€ ìš©ì–´ DBì— ì˜ì¡´ì 
        - ì˜ë¯¸ì  ìœ ì‚¬ì„± ë¯¸ë°˜ì˜
        """
        # DBì—ì„œ í‘œì¤€ ìš©ì–´ ë¡œë“œ
        db_terms = self._get_db_terms(category)
        
        # LLM í”„ë¡¬í”„íŠ¸ ìƒì„± ë° í˜¸ì¶œ
        prompt = self._create_normalization_prompt(term, db_terms, category)
        result = self.client.chat.completions.create(...)
        
        return normalized_term, confidence
```

#### Production ë²¡í„° ê¸°ë°˜ ì •ê·œí™”
```python
class VectorBasedNormalizer:
    """
    ë²¡í„° ì„ë² ë”© ê¸°ë°˜ ì •ê·œí™” ì—”ì§„
    
    ê°œì„ ì‚¬í•­:
    - ì‹¤ì‹œê°„ ì˜ë¯¸ì  ìœ ì‚¬ì„± ê³„ì‚°
    - ì¼ê´€ëœ ì •ê·œí™” ê²°ê³¼
    - ìƒˆë¡œìš´ ìš©ì–´ ìë™ í•™ìŠµ
    - ë‹¤êµ­ì–´ ì§€ì› í–¥ìƒ
    """
    
    def __init__(self, vector_db: VectorDatabase, embedding_model: str):
        self.vector_db = vector_db
        self.embedding_model = SentenceTransformer(embedding_model)
        self.similarity_threshold = 0.85
        
    async def normalize_term(self, term: str, category: str) -> Tuple[str, float]:
        """
        ë²¡í„° ìœ ì‚¬ì„± ê¸°ë°˜ ìš©ì–´ ì •ê·œí™”
        
        AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
        - ì„ë² ë”© ëª¨ë¸ êµì²´ ê°€ëŠ¥í•œ êµ¬ì¡°
        - ìœ ì‚¬ë„ ì„ê³„ê°’ ë™ì  ì¡°ì ˆ
        - ì‹¤ì‹œê°„ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
        """
        # ì…ë ¥ ìš©ì–´ ì„ë² ë”© ìƒì„±
        term_embedding = self.embedding_model.encode(term)
        
        # ë²¡í„° DBì—ì„œ ìœ ì‚¬ ìš©ì–´ ê²€ìƒ‰
        similar_terms = await self.vector_db.search(
            embedding=term_embedding,
            collection=f"standard_terms_{category}",
            top_k=5,
            threshold=self.similarity_threshold
        )
        
        if similar_terms:
            # ê°€ì¥ ìœ ì‚¬í•œ í‘œì¤€ ìš©ì–´ ë°˜í™˜
            best_match = similar_terms[0]
            confidence = best_match.similarity
            
            # í•™ìŠµ ë°ì´í„°ë¡œ í™œìš©
            await self._log_normalization_result(term, best_match.text, confidence)
            
            return best_match.text, confidence
        else:
            # í‘œì¤€ ìš©ì–´ê°€ ì—†ëŠ” ê²½ìš° LLM í´ë°±
            return await self._llm_fallback_normalization(term, category)
    
    async def _log_normalization_result(self, original: str, normalized: str, confidence: float):
        """
        ì •ê·œí™” ê²°ê³¼ ë¡œê¹… - ì§€ì†ì  í•™ìŠµìš©
        
        ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
        - ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘
        - ëª¨ë¸ ì¬í•™ìŠµ ë°ì´í„° ì¶•ì 
        - ì„±ëŠ¥ ê°œì„  ì§€í‘œ ìƒì„±
        """
        log_entry = {
            "timestamp": datetime.utcnow(),
            "original_term": original,
            "normalized_term": normalized,
            "confidence": confidence,
            "model_version": self.embedding_model.get_model_name()
        }
        
        # ë¹„ë™ê¸° ë¡œê¹… (ì„±ëŠ¥ ì˜í–¥ ìµœì†Œí™”)
        await self.logger.log_async(log_entry)
```

### 2. ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬í˜„

```python
class ProductionVectorDatabase:
    """
    Productionìš© ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
    
    AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
    - ë‹¤ì–‘í•œ ë²¡í„° DB ë°±ì—”ë“œ ì§€ì› (FAISS, Qdrant, Weaviate)
    - ì‹¤ì‹œê°„ ì¸ë±ì‹± ë° ê²€ìƒ‰
    - ìŠ¤ì¼€ì¼ë§ ë° ìƒ¤ë”© ì§€ì›
    """
    
    def __init__(self, config: VectorDBConfig):
        self.config = config
        self.backend = self._initialize_backend()
        self.collections = {}
    
    def _initialize_backend(self):
        """ë²¡í„° DB ë°±ì—”ë“œ ì´ˆê¸°í™”"""
        if self.config.backend == "qdrant":
            from qdrant_client import QdrantClient
            return QdrantClient(
                host=self.config.host,
                port=self.config.port,
                api_key=self.config.api_key
            )
        elif self.config.backend == "weaviate":
            import weaviate
            return weaviate.Client(
                url=self.config.url,
                auth_client_secret=weaviate.AuthApiKey(api_key=self.config.api_key)
            )
        else:  # FAISS ê¸°ë³¸ê°’
            import faiss
            return faiss.IndexFlatIP(self.config.dimension)
    
    async def create_collection(self, name: str, schema: Dict):
        """
        ì»¬ë ‰ì…˜ ìƒì„± - ì¹´í…Œê³ ë¦¬ë³„ ë²¡í„° ì €ì¥ì†Œ
        
        ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
        - ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ë°ì´í„° ê²€ì¦
        - ì¸ë±ìŠ¤ íƒ€ì… ìµœì í™”
        - ë°±ì—… ë° ë³µêµ¬ ì§€ì›
        """
        if self.config.backend == "qdrant":
            await self.backend.recreate_collection(
                collection_name=name,
                vectors_config=models.VectorParams(
                    size=self.config.dimension,
                    distance=models.Distance.COSINE
                )
            )
        
        self.collections[name] = schema
        logger.info(f"Created collection: {name}")
    
    async def upsert_vectors(self, collection: str, vectors: List[VectorData]):
        """
        ë²¡í„° ë°ì´í„° ì‚½ì…/ì—…ë°ì´íŠ¸
        
        AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
        - ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
        - ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬
        - ë©”íƒ€ë°ì´í„° ì¸ë±ì‹±
        """
        if self.config.backend == "qdrant":
            points = [
                models.PointStruct(
                    id=vector.id,
                    vector=vector.embedding,
                    payload=vector.metadata
                )
                for vector in vectors
            ]
            
            await self.backend.upsert(
                collection_name=collection,
                points=points
            )
    
    async def search(self, 
                    embedding: List[float], 
                    collection: str, 
                    top_k: int = 10, 
                    threshold: float = 0.8,
                    filters: Optional[Dict] = None) -> List[SearchResult]:
        """
        ë²¡í„° ìœ ì‚¬ì„± ê²€ìƒ‰
        
        ì„±ëŠ¥ ìµœì í™”:
        - í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)
        - í•„í„°ë§ ì¡°ê±´ ìµœì í™”
        - ê²°ê³¼ ìºì‹±
        """
        if self.config.backend == "qdrant":
            search_result = await self.backend.search(
                collection_name=collection,
                query_vector=embedding,
                limit=top_k,
                score_threshold=threshold,
                query_filter=self._convert_filters(filters) if filters else None
            )
            
            return [
                SearchResult(
                    id=hit.id,
                    text=hit.payload.get("text", ""),
                    similarity=hit.score,
                    metadata=hit.payload
                )
                for hit in search_result
                if hit.score >= threshold
            ]
```

### 3. ì„ë² ë”© ì„œë¹„ìŠ¤

```python
class EmbeddingService:
    """
    ì„ë² ë”© ìƒì„± ë° ê´€ë¦¬ ì„œë¹„ìŠ¤
    
    ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
    - ë‹¤ì¤‘ ëª¨ë¸ ì§€ì› (Sentence Transformers, OpenAI, Cohere)
    - ë°°ì¹˜ ì²˜ë¦¬ ë° ìºì‹±
    - ëª¨ë¸ ë²„ì „ ê´€ë¦¬
    """
    
    def __init__(self, config: EmbeddingConfig):
        self.models = {}
        self.cache = EmbeddingCache(max_size=10000)
        self.load_models(config.models)
    
    def load_models(self, model_configs: List[ModelConfig]):
        """ë‹¤ì¤‘ ì„ë² ë”© ëª¨ë¸ ë¡œë”©"""
        for config in model_configs:
            if config.provider == "sentence_transformers":
                model = SentenceTransformer(config.model_name)
                if config.gpu_device >= 0:
                    model = model.to(f"cuda:{config.gpu_device}")
                self.models[config.name] = model
            elif config.provider == "openai":
                self.models[config.name] = OpenAIEmbedding(config.api_key)
    
    async def encode_batch(self, 
                          texts: List[str], 
                          model_name: str = "default",
                          normalize: bool = True) -> List[List[float]]:
        """
        ë°°ì¹˜ ì„ë² ë”© ìƒì„±
        
        AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
        - GPU ë©”ëª¨ë¦¬ ìµœì í™”
        - ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì ˆ
        - ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
        """
        # ìºì‹œ í™•ì¸
        cache_keys = [f"{model_name}:{hash(text)}" for text in texts]
        cached_results = []
        uncached_texts = []
        uncached_indices = []
        
        for i, (text, cache_key) in enumerate(zip(texts, cache_keys)):
            cached_embedding = self.cache.get(cache_key)
            if cached_embedding is not None:
                cached_results.append((i, cached_embedding))
            else:
                uncached_texts.append(text)
                uncached_indices.append(i)
        
        # ìºì‹œë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ë§Œ ì²˜ë¦¬
        if uncached_texts:
            model = self.models[model_name]
            embeddings = model.encode(
                uncached_texts,
                batch_size=32,
                normalize_embeddings=normalize,
                show_progress_bar=False
            )
            
            # ìºì‹œì— ì €ì¥
            for text, embedding, cache_key in zip(uncached_texts, embeddings, 
                                                 [cache_keys[i] for i in uncached_indices]):
                self.cache.set(cache_key, embedding.tolist())
        
        # ê²°ê³¼ ì¬ì¡°ë¦½
        results = [None] * len(texts)
        for i, embedding in cached_results:
            results[i] = embedding
        for i, embedding in zip(uncached_indices, embeddings):
            results[i] = embedding.tolist()
        
        return results
```

---

## ğŸ“Š ì„±ëŠ¥ í‰ê°€ ì‹œìŠ¤í…œ ì„¤ê³„

### 1. íŒŒì‹± ë¡œì§ í‰ê°€

```python
class ParsingEvaluator:
    """
    íŒŒì‹± ë¡œì§ ì„±ëŠ¥ í‰ê°€ê¸°
    
    AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
    - ì •í™•ë„, ì¬í˜„ìœ¨, F1 ìŠ¤ì½”ì–´ ê³„ì‚°
    - ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ë¶„ì„
    - ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """
    
    def __init__(self, ground_truth_dataset: str):
        self.ground_truth = self.load_ground_truth(ground_truth_dataset)
        self.metrics_collector = MetricsCollector()
    
    async def evaluate_parsing_accuracy(self, 
                                       parser: InputParser, 
                                       test_cases: List[TestCase]) -> EvaluationReport:
        """
        íŒŒì‹± ì •í™•ë„ í‰ê°€
        
        ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
        - A/B í…ŒìŠ¤íŠ¸ ì§€ì›
        - ì‹¤ì‹œê°„ í‰ê°€ ê²°ê³¼ ìˆ˜ì§‘
        - ìë™ ì•Œë¦¼ ì‹œìŠ¤í…œ ì—°ë™
        """
        results = []
        total_time = 0
        
        for test_case in test_cases:
            start_time = time.time()
            
            # íŒŒì‹± ì‹¤í–‰
            parsed_result = await parser.parse_input_with_context(
                user_input=test_case.input,
                conversation_history=test_case.history,
                session_id=test_case.session_id
            )
            
            parsing_time = time.time() - start_time
            total_time += parsing_time
            
            # ì •í™•ë„ ê³„ì‚°
            accuracy_score = self._calculate_parsing_accuracy(
                parsed_result, test_case.expected_output
            )
            
            results.append(ParsingResult(
                test_case_id=test_case.id,
                accuracy=accuracy_score,
                parsing_time=parsing_time,
                confidence=parsed_result.confidence,
                scenario=parsed_result.scenario
            ))
        
        # ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
        report = EvaluationReport(
            parser_version=parser.get_version(),
            test_cases_count=len(test_cases),
            average_accuracy=np.mean([r.accuracy for r in results]),
            average_parsing_time=total_time / len(test_cases),
            scenario_breakdown=self._analyze_by_scenario(results),
            timestamp=datetime.utcnow()
        )
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        await self.metrics_collector.record_evaluation(report)
        
        return report
    
    def _calculate_parsing_accuracy(self, 
                                   parsed: ParsedInput, 
                                   expected: ParsedInput) -> float:
        """
        íŒŒì‹± ê²°ê³¼ ì •í™•ë„ ê³„ì‚°
        
        í‰ê°€ ê¸°ì¤€:
        - í•„ë“œë³„ ê°€ì¤‘ì¹˜ ì ìš©
        - ë¶€ë¶„ ë§¤ì¹­ ì ìˆ˜ ë°˜ì˜
        - ì‹ ë¢°ë„ ì ìˆ˜ ì¡°í•©
        """
        scores = []
        weights = {"location": 0.3, "equipment_type": 0.3, "status_code": 0.25, "priority": 0.15}
        
        for field, weight in weights.items():
            parsed_value = getattr(parsed, field, None)
            expected_value = getattr(expected, field, None)
            
            if parsed_value == expected_value:
                scores.append(1.0 * weight)
            elif parsed_value and expected_value:
                # ë¶€ë¶„ ë§¤ì¹­ ì ìˆ˜
                similarity = SequenceMatcher(None, 
                                           str(parsed_value).lower(), 
                                           str(expected_value).lower()).ratio()
                scores.append(similarity * weight)
            else:
                scores.append(0.0)
        
        return sum(scores)
```

### 2. ì¶”ì²œ ë¡œì§ í‰ê°€

```python
class RecommendationEvaluator:
    """
    ì¶”ì²œ ë¡œì§ ì„±ëŠ¥ í‰ê°€ê¸°
    
    AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
    - ì¶”ì²œ ì •í™•ë„ (Precision@K, Recall@K)
    - ë‹¤ì–‘ì„± ì§€í‘œ (Diversity, Coverage)
    - ì‚¬ìš©ì ë§Œì¡±ë„ ì˜ˆì¸¡
    """
    
    def __init__(self, recommendation_engine: RecommendationEngine):
        self.engine = recommendation_engine
        self.evaluation_metrics = RecommendationMetrics()
    
    async def evaluate_recommendation_quality(self, 
                                            test_queries: List[QueryTestCase]) -> RecommendationReport:
        """
        ì¶”ì²œ í’ˆì§ˆ í‰ê°€
        
        ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
        - ë‹¤ì¤‘ í‰ê°€ ì§€í‘œ ë™ì‹œ ê³„ì‚°
        - ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥ ë³€í™” ì¶”ì 
        - ì‹¤ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜
        """
        results = []
        
        for query_case in test_queries:
            start_time = time.time()
            
            # ì¶”ì²œ ìƒì„±
            recommendations = await self.engine.get_recommendations(
                parsed_input=query_case.parsed_input,
                limit=10
            )
            
            recommendation_time = time.time() - start_time
            
            # ë‹¤ì¤‘ ì§€í‘œ ê³„ì‚°
            metrics = await self._calculate_recommendation_metrics(
                recommendations=recommendations,
                relevant_items=query_case.relevant_items,
                query_context=query_case.context
            )
            
            results.append(RecommendationResult(
                query_id=query_case.id,
                metrics=metrics,
                recommendation_time=recommendation_time,
                recommendation_count=len(recommendations)
            ))
        
        # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        report = RecommendationReport(
            engine_version=self.engine.get_version(),
            evaluation_timestamp=datetime.utcnow(),
            query_count=len(test_queries),
            average_precision_at_5=np.mean([r.metrics.precision_at_5 for r in results]),
            average_recall_at_5=np.mean([r.metrics.recall_at_5 for r in results]),
            average_recommendation_time=np.mean([r.recommendation_time for r in results]),
            diversity_score=np.mean([r.metrics.diversity for r in results])
        )
        
        return report
    
    async def _calculate_recommendation_metrics(self, 
                                              recommendations: List[Recommendation],
                                              relevant_items: List[str],
                                              query_context: Dict) -> RecommendationMetrics:
        """ì¶”ì²œ ì§€í‘œ ê³„ì‚°"""
        recommended_ids = [rec.itemno for rec in recommendations]
        
        # Precision@K ê³„ì‚°
        precision_at_5 = len(set(recommended_ids[:5]) & set(relevant_items)) / min(5, len(recommended_ids))
        precision_at_10 = len(set(recommended_ids[:10]) & set(relevant_items)) / min(10, len(recommended_ids))
        
        # Recall@K ê³„ì‚°
        recall_at_5 = len(set(recommended_ids[:5]) & set(relevant_items)) / len(relevant_items) if relevant_items else 0
        recall_at_10 = len(set(recommended_ids[:10]) & set(relevant_items)) / len(relevant_items) if relevant_items else 0
        
        # ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
        diversity = self._calculate_diversity(recommendations)
        
        return RecommendationMetrics(
            precision_at_5=precision_at_5,
            precision_at_10=precision_at_10,
            recall_at_5=recall_at_5,
            recall_at_10=recall_at_10,
            diversity=diversity,
            average_score=np.mean([rec.score for rec in recommendations])
        )
```

### 3. ë²¡í„° ì„ë² ë”© í‰ê°€

```python
class VectorEmbeddingEvaluator:
    """
    ë²¡í„° ì„ë² ë”© ì‹œìŠ¤í…œ í‰ê°€ê¸°
    
    AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
    - ì„ë² ë”© í’ˆì§ˆ í‰ê°€ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„, êµ°ì§‘í™” ì„±ëŠ¥)
    - ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ (ê²€ìƒ‰ ì†ë„, ì •í™•ë„)
    - ëª¨ë¸ ë¹„êµ í‰ê°€
    """
    
    def __init__(self, vector_db: ProductionVectorDatabase, embedding_service: EmbeddingService):
        self.vector_db = vector_db
        self.embedding_service = embedding_service
        self.benchmark_datasets = self.load_benchmark_datasets()
    
    async def evaluate_embedding_quality(self, 
                                        model_name: str, 
                                        test_datasets: List[EmbeddingTestSet]) -> EmbeddingReport:
        """
        ì„ë² ë”© í’ˆì§ˆ ì¢…í•© í‰ê°€
        
        ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
        - ë‹¤ì¤‘ í‰ê°€ ë°ì´í„°ì…‹ ì§€ì›
        - ì‹¤ì‹œê°„ í‰ê°€ ê²°ê³¼ ë¹„êµ
        - ëª¨ë¸ êµì²´ ì˜ì‚¬ê²°ì • ì§€ì›
        """
        evaluation_results = []
        
        for dataset in test_datasets:
            # ì„ë² ë”© ìƒì„±
            embeddings = await self.embedding_service.encode_batch(
                texts=dataset.texts,
                model_name=model_name
            )
            
            # ìœ ì‚¬ë„ í‰ê°€
            similarity_scores = self._evaluate_similarity_quality(
                embeddings=embeddings,
                similarity_pairs=dataset.similarity_pairs
            )
            
            # ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€
            search_performance = await self._evaluate_search_performance(
                embeddings=embeddings,
                queries=dataset.search_queries,
                model_name=model_name
            )
            
            # êµ°ì§‘í™” ì„±ëŠ¥ í‰ê°€
            clustering_performance = self._evaluate_clustering_quality(
                embeddings=embeddings,
                true_labels=dataset.labels
            )
            
            evaluation_results.append(EmbeddingDatasetResult(
                dataset_name=dataset.name,
                similarity_correlation=similarity_scores.correlation,
                search_accuracy=search_performance.accuracy,
                clustering_score=clustering_performance.adjusted_rand_score,
                embedding_dimension=len(embeddings[0]) if embeddings else 0
            ))
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_embedding_score(evaluation_results)
        
        return EmbeddingReport(
            model_name=model_name,
            evaluation_timestamp=datetime.utcnow(),
            dataset_results=evaluation_results,
            overall_score=overall_score,
            performance_summary=self._generate_performance_summary(evaluation_results)
        )
    
    async def _evaluate_search_performance(self, 
                                         embeddings: List[List[float]], 
                                         queries: List[SearchQuery],
                                         model_name: str) -> SearchPerformanceResult:
        """ë²¡í„° ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€"""
        search_times = []
        accuracy_scores = []
        
        for query in queries:
            start_time = time.time()
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = await self.embedding_service.encode_batch(
                texts=[query.text],
                model_name=model_name
            )
            
            # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
            search_results = await self.vector_db.search(
                embedding=query_embedding[0],
                collection=query.collection,
                top_k=query.top_k,
                threshold=query.threshold
            )
            
            search_time = time.time() - start_time
            search_times.append(search_time)
            
            # ê²€ìƒ‰ ì •í™•ë„ ê³„ì‚°
            relevant_ids = set(query.relevant_document_ids)
            retrieved_ids = set([result.id for result in search_results])
            
            precision = len(relevant_ids & retrieved_ids) / len(retrieved_ids) if retrieved_ids else 0
            recall = len(relevant_ids & retrieved_ids) / len(relevant_ids) if relevant_ids else 0
            f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            
            accuracy_scores.append(f1_score)
        
        return SearchPerformanceResult(
            average_search_time=np.mean(search_times),
            accuracy=np.mean(accuracy_scores),
            queries_per_second=1.0 / np.mean(search_times) if search_times else 0
        )
```

### 4. ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
class SystemPerformanceMonitor:
    """
    ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    
    ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
    - ì‹¤ì‹œê°„ ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘
    - ì•ŒëŒ ë° ìë™ ìŠ¤ì¼€ì¼ë§ ì—°ë™
    - ì„±ëŠ¥ ë³‘ëª© ì§€ì  ìë™ íƒì§€
    """
    
    def __init__(self, config: MonitoringConfig):
        self.config = config
        self.metrics_collector = MetricsCollector()
        self.alerting_system = AlertingSystem(config.alert_config)
        self.performance_baseline = self.load_baseline_metrics()
    
    async def monitor_request_pipeline(self, 
                                     user_input: str, 
                                     session_id: str) -> PipelineMetrics:
        """
        ìš”ì²­ íŒŒì´í”„ë¼ì¸ ì „ì²´ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        
        AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
        - ê° ë‹¨ê³„ë³„ ì„±ëŠ¥ ì¸¡ì •
        - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        - ì—ëŸ¬ìœ¨ ë° ì¬ì‹œë„ íšŸìˆ˜ ê¸°ë¡
        """
        pipeline_start = time.time()
        metrics = PipelineMetrics(request_id=str(uuid.uuid4()))
        
        try:
            # 1. íŒŒì‹± ë‹¨ê³„
            parsing_start = time.time()
            parsed_input = await self._monitor_parsing_stage(user_input, session_id)
            metrics.parsing_time = time.time() - parsing_start
            
            # 2. ì •ê·œí™” ë‹¨ê³„
            normalization_start = time.time()
            normalized_data = await self._monitor_normalization_stage(parsed_input)
            metrics.normalization_time = time.time() - normalization_start
            
            # 3. ë²¡í„° ê²€ìƒ‰ ë‹¨ê³„ (í™œì„±í™”ëœ ê²½ìš°)
            if self.config.vector_search_enabled:
                vector_search_start = time.time()
                vector_results = await self._monitor_vector_search_stage(normalized_data)
                metrics.vector_search_time = time.time() - vector_search_start
            
            # 4. ì¶”ì²œ ë‹¨ê³„
            recommendation_start = time.time()
            recommendations = await self._monitor_recommendation_stage(normalized_data)
            metrics.recommendation_time = time.time() - recommendation_start
            
            # 5. ì‘ë‹µ ìƒì„± ë‹¨ê³„
            response_start = time.time()
            response = await self._monitor_response_generation_stage(recommendations)
            metrics.response_generation_time = time.time() - response_start
            
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹œê°„
            metrics.total_pipeline_time = time.time() - pipeline_start
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìˆ˜ì§‘
            metrics.memory_usage = self._get_current_memory_usage()
            
            # ì„±ëŠ¥ ê¸°ì¤€ì„ ê³¼ ë¹„êµ
            performance_status = self._compare_with_baseline(metrics)
            
            # ì•ŒëŒ ì²´í¬
            if performance_status.requires_alert:
                await self.alerting_system.send_performance_alert(metrics, performance_status)
            
            # ë©”íŠ¸ë¦­ ì €ì¥
            await self.metrics_collector.store_pipeline_metrics(metrics)
            
            return metrics
            
        except Exception as e:
            metrics.error_occurred = True
            metrics.error_message = str(e)
            metrics.total_pipeline_time = time.time() - pipeline_start
            
            await self.alerting_system.send_error_alert(metrics, e)
            raise
    
    async def _monitor_parsing_stage(self, user_input: str, session_id: str) -> ParsedInput:
        """íŒŒì‹± ë‹¨ê³„ ëª¨ë‹ˆí„°ë§"""
        with self.metrics_collector.timer("parsing_stage"):
            parser = InputParser()
            return await parser.parse_input_with_context(user_input, [], session_id)
    
    def _compare_with_baseline(self, metrics: PipelineMetrics) -> PerformanceStatus:
        """ì„±ëŠ¥ ê¸°ì¤€ì„ ê³¼ ë¹„êµ"""
        status = PerformanceStatus()
        
        # íŒŒì‹± ì‹œê°„ ì²´í¬
        if metrics.parsing_time > self.performance_baseline.parsing_time * 1.5:
            status.parsing_slow = True
            status.requires_alert = True
        
        # ì „ì²´ ì‘ë‹µ ì‹œê°„ ì²´í¬
        if metrics.total_pipeline_time > self.performance_baseline.total_time * 2.0:
            status.overall_slow = True
            status.requires_alert = True
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
        if metrics.memory_usage > self.performance_baseline.memory_usage * 1.8:
            status.memory_high = True
            status.requires_alert = True
        
        return status
    
    async def generate_performance_report(self, 
                                        time_range: TimeRange) -> PerformanceReport:
        """
        ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        
        ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
        - ì‹œê°„ëŒ€ë³„ ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        - ë³‘ëª© ì§€ì  ìë™ ì‹ë³„
        - ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±
        """
        metrics_data = await self.metrics_collector.get_metrics_in_range(time_range)
        
        # í‰ê·  ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        avg_metrics = self._calculate_average_metrics(metrics_data)
        
        # ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        trends = self._analyze_performance_trends(metrics_data)
        
        # ë³‘ëª© ì§€ì  ì‹ë³„
        bottlenecks = self._identify_bottlenecks(metrics_data)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_optimization_recommendations(
            avg_metrics, trends, bottlenecks
        )
        
        return PerformanceReport(
            time_range=time_range,
            total_requests=len(metrics_data),
            average_metrics=avg_metrics,
            performance_trends=trends,
            identified_bottlenecks=bottlenecks,
            optimization_recommendations=recommendations,
            generated_at=datetime.utcnow()
        )
```

---

## ğŸ‘¥ ì—­í• ë³„ ê°œë°œ ê°€ì´ë“œ

### AI ì—°êµ¬ì› (í”„ë¡œì íŠ¸íŒ€) ê°€ì´ë“œ

#### 1. íŒŒì¼ ë° í´ë˜ìŠ¤ ì—°ê³„ ê´€ê³„

```python
"""
AI ì—°êµ¬ì›ì„ ìœ„í•œ ì½”ë“œ êµ¬ì¡° ê°€ì´ë“œ

=== í•µì‹¬ íŒŒì¼ êµ¬ì¡° ===
backend/app/
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ parser.py          # ì‚¬ìš©ì ì…ë ¥ íŒŒì‹± (ì‹œë‚˜ë¦¬ì˜¤ 1, 2 ë¶„ê¸°)
â”œâ”€â”€ logic/
â”‚   â”œâ”€â”€ normalizer.py      # ìš©ì–´ ì •ê·œí™” (LLM ê¸°ë°˜ â†’ ë²¡í„° ê¸°ë°˜ ì „í™˜ ì˜ˆì •)
â”‚   â””â”€â”€ recommender.py     # ì¶”ì²œ ì—”ì§„ (ìœ ì‚¬ë„ ê³„ì‚° ë° ì¶”ì²œ ìƒì„±)
â”œâ”€â”€ database.py            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (SQLite â†’ Azure SQL ì „í™˜ ì˜ˆì •)
â”œâ”€â”€ session_manager.py     # ì„¸ì…˜ ê´€ë¦¬ (ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ìœ ì§€)
â””â”€â”€ models.py             # ë°ì´í„° ëª¨ë¸ ì •ì˜

=== ì—°ê³„ ê´€ê³„ ===
1. ì‚¬ìš©ì ì…ë ¥ â†’ parser.py â†’ normalizer.py â†’ recommender.py â†’ ì‘ë‹µ
2. ì„¸ì…˜ ê´€ë¦¬: session_manager.pyê°€ ëª¨ë“  ë‹¨ê³„ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
3. ë°ì´í„° ì ‘ê·¼: database.pyê°€ ëª¨ë“  ëª¨ë“ˆì—ì„œ ê³µí†µ ì‚¬ìš©
"""

# ì˜ˆì‹œ: íŒŒì‹± ë¡œì§ ì‹¤í—˜ ë° ê°œì„ 
class ParsingExperiment:
    """
    AI ì—°êµ¬ì›ì„ ìœ„í•œ íŒŒì‹± ë¡œì§ ì‹¤í—˜ í´ë˜ìŠ¤
    
    ì‹¤í—˜ ê°€ëŠ¥í•œ ì˜ì—­:
    1. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê°œì„ 
    2. LLM ëª¨ë¸ êµì²´ í…ŒìŠ¤íŠ¸
    3. ì‹œë‚˜ë¦¬ì˜¤ ë¶„ê¸° ë¡œì§ ê°œì„ 
    4. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
    """
    
    def __init__(self):
        # í˜„ì¬ í”„ë¡œë•ì…˜ íŒŒì„œ ë¡œë“œ
        self.production_parser = InputParser()
        
        # ì‹¤í—˜ìš© íŒŒì„œ ì„¤ì •
        self.experimental_parsers = {
            "gpt4": self._create_gpt4_parser(),
            "local_mistral": self._create_local_llm_parser("mistral-7b"),
            "local_qwen": self._create_local_llm_parser("qwen3-14b")
        }
    
    def compare_parsing_models(self, test_inputs: List[str]) -> ComparisonReport:
        """
        ë‹¤ì–‘í•œ íŒŒì‹± ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        
        ì‚¬ìš©ë²•:
        1. test_inputsì— ë‹¤ì–‘í•œ ì‚¬ìš©ì ì…ë ¥ ì¤€ë¹„
        2. ê° ëª¨ë¸ë³„ íŒŒì‹± ê²°ê³¼ ë° ì„±ëŠ¥ ë¹„êµ
        3. ê²°ê³¼ ë¶„ì„ ë° ìµœì  ëª¨ë¸ ì„ íƒ
        
        ì£¼ì˜ì‚¬í•­:
        - ì‹¤ì œ í”„ë¡œë•ì…˜ ì½”ë“œ ìˆ˜ì • ì—†ì´ ì‹¤í—˜ë§Œ ì§„í–‰
        - ê²°ê³¼ê°€ ìš°ìˆ˜í•œ ê²½ìš° ê°œë°œíŒ€ê³¼ í˜‘ì˜ í›„ ì ìš©
        """
        results = {}
        
        for model_name, parser in self.experimental_parsers.items():
            model_results = []
            
            for user_input in test_inputs:
                start_time = time.time()
                parsed_result = parser.parse_input(user_input)
                parsing_time = time.time() - start_time
                
                model_results.append({
                    "input": user_input,
                    "parsed_result": parsed_result,
                    "parsing_time": parsing_time,
                    "confidence": parsed_result.confidence
                })
            
            results[model_name] = model_results
        
        return self._generate_comparison_report(results)
```

#### 2. ì‹¤í—˜ ì›Œí¬í”Œë¡œìš° ê°€ì´ë“œ

```python
"""
AI ì—°êµ¬ì› ì‹¤í—˜ ì›Œí¬í”Œë¡œìš°

=== ë‹¨ê³„ë³„ ì‹¤í—˜ ê°€ì´ë“œ ===

1. ë°ì´í„° ì¤€ë¹„ ë‹¨ê³„
   - notebooks/ì—ì„œ ì‹¤í—˜ìš© ë°ì´í„°ì…‹ ì¤€ë¹„
   - ì‹¤ì œ ì‚¬ìš©ì ì…ë ¥ ìƒ˜í”Œ ìˆ˜ì§‘
   - Ground Truth ë°ì´í„° ìƒì„±

2. ëª¨ë¸ ì‹¤í—˜ ë‹¨ê³„
   - ë…¸íŠ¸ë¶ì—ì„œ ê°œë³„ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
   - ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘ ë° ë¶„ì„
   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

3. í†µí•© í…ŒìŠ¤íŠ¸ ë‹¨ê³„
   - ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
   - A/B í…ŒìŠ¤íŠ¸ ì„¤ê³„ ë° ì‹¤í–‰
   - ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘

4. í”„ë¡œë•ì…˜ ë°˜ì˜ ë‹¨ê³„
   - ê°œë°œíŒ€ê³¼ ê²°ê³¼ ê³µìœ 
   - ì½”ë“œ ë¦¬ë·° ë° í†µí•© ê³„íš ìˆ˜ë¦½
   - ì ì§„ì  ë°°í¬ ì „ëµ ìˆ˜ë¦½
"""

class ExperimentTracker:
    """
    ì‹¤í—˜ ì¶”ì  ë° ê´€ë¦¬ í´ë˜ìŠ¤
    
    ëª©ì : AI ì—°êµ¬ì›ì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬
    """
    
    def __init__(self, experiment_name: str):
        self.experiment_name = experiment_name
        self.results = []
        self.metadata = {
            "created_at": datetime.utcnow(),
            "researcher": os.getenv("USER", "unknown"),
            "version": self._get_current_version()
        }
    
    def log_experiment(self, 
                      module: str, 
                      configuration: Dict, 
                      metrics: Dict,
                      notes: str = ""):
        """
        ì‹¤í—˜ ê²°ê³¼ ë¡œê¹…
        
        ì‚¬ìš©ë²•:
        tracker.log_experiment(
            module="parser",
            configuration={"model": "gpt-4", "temperature": 0.1},
            metrics={"accuracy": 0.95, "speed": 1.2},
            notes="í”„ë¡¬í”„íŠ¸ ìµœì í™” í›„ ì •í™•ë„ 5% í–¥ìƒ"
        )
        """
        experiment_result = {
            "timestamp": datetime.utcnow(),
            "module": module,
            "configuration": configuration,
            "metrics": metrics,
            "notes": notes
        }
        
        self.results.append(experiment_result)
        
        # ìë™ ì €ì¥ (ì‹¤í—˜ ê²°ê³¼ ì†ì‹¤ ë°©ì§€)
        self._save_to_file()
    
    def compare_experiments(self, metric_name: str) -> DataFrame:
        """ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë¶„ì„"""
        df = pd.DataFrame(self.results)
        return df.groupby('module')[f'metrics.{metric_name}'].describe()
    
    def _save_to_file(self):
        """ì‹¤í—˜ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        filename = f"experiments/{self.experiment_name}_{datetime.now().strftime('%Y%m%d')}.json"
        with open(filename, 'w') as f:
            json.dump({
                "metadata": self.metadata,
                "results": self.results
            }, f, indent=2, default=str)
```

### ê°œë°œíŒ€ ê°€ì´ë“œ

#### 1. LangGraph ì•„í‚¤í…ì²˜ êµ¬í˜„ ê°€ì´ë“œ

```python
"""
ê°œë°œíŒ€ì„ ìœ„í•œ LangGraph ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

=== ì•„í‚¤í…ì²˜ ì„¤ê³„ ì›ì¹™ ===
1. í˜„ì¬ í•¨ìˆ˜ ê¸°ë°˜ â†’ ê·¸ë˜í”„ ë…¸ë“œ ê¸°ë°˜ ì „í™˜
2. ìƒíƒœ ê´€ë¦¬ ì¤‘ì•™í™” (StateManager)
3. ì¡°ê±´ë¶€ ë¶„ê¸°ë¥¼ í†µí•œ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬
4. ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘

=== ì£¼ìš” êµ¬í˜„ í¬ì¸íŠ¸ ===
"""

class ProductionWorkflowBuilder:
    """
    Productionìš© LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë”
    
    ê°œë°œíŒ€ ì£¼ì˜ì‚¬í•­:
    1. ê° ë…¸ë“œëŠ” ë…ë¦½ì ìœ¼ë¡œ ìŠ¤ì¼€ì¼ ê°€ëŠ¥í•´ì•¼ í•¨
    2. ìƒíƒœëŠ” Redis/Azure Cacheì— ì €ì¥í•˜ì—¬ ë¶„ì‚° ì²˜ë¦¬ ì§€ì›
    3. ì—ëŸ¬ ë°œìƒ ì‹œ ì¬ì‹œë„ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ í•„ìˆ˜
    4. ëª¨ë“  ë…¸ë“œì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ë¡œê¹…
    """
    
    def __init__(self, config: ProductionConfig):
        self.config = config
        self.state_store = self._initialize_state_store()
        self.metrics_collector = MetricsCollector()
        self.logger = self._setup_logging()
    
    def create_production_workflow(self) -> CompiledGraph:
        """
        Productionìš© ì›Œí¬í”Œë¡œìš° ìƒì„±
        
        êµ¬í˜„ ê³ ë ¤ì‚¬í•­:
        1. ë…¸ë“œ ê°„ ë¹„ë™ê¸° í†µì‹ 
        2. ë°±í”„ë ˆì…” ì²˜ë¦¬ (ê³¼ë¶€í•˜ ì‹œ ìš”ì²­ ì œí•œ)
        3. ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´ ì ìš©
        4. í—¬ìŠ¤ì²´í¬ ë° ëª¨ë‹ˆí„°ë§ ì—”ë“œí¬ì¸íŠ¸
        """
        workflow = StateGraph(ProductionWorkflowState)
        
        # ë…¸ë“œ ì •ì˜ ë° ì¶”ê°€
        nodes = {
            "input_validation": self._create_input_validation_node(),
            "parsing": self._create_parsing_node(),
            "normalization": self._create_normalization_node(),
            "vector_search": self._create_vector_search_node(),
            "recommendation": self._create_recommendation_node(),
            "response_generation": self._create_response_generation_node(),
            "quality_check": self._create_quality_check_node(),
            "error_handler": self._create_error_handler_node()
        }
        
        for name, node_func in nodes.items():
            workflow.add_node(name, node_func)
        
        # ì—£ì§€ ì •ì˜ (ì¡°ê±´ë¶€ ë¶„ê¸° í¬í•¨)
        self._define_workflow_edges(workflow)
        
        # ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
        workflow = self._add_middleware(workflow)
        
        return workflow.compile()
    
    def _create_parsing_node(self) -> Callable:
        """
        íŒŒì‹± ë…¸ë“œ ìƒì„±
        
        êµ¬í˜„ ì„¸ë¶€ì‚¬í•­:
        1. ë¡œì»¬ LLM ì—”ë“œí¬ì¸íŠ¸ ì—°ë™
        2. ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
        3. íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ë¡œì§
        4. ê²°ê³¼ ìºì‹±
        """
        async def parsing_node(state: ProductionWorkflowState) -> ProductionWorkflowState:
            try:
                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œì‘
                with self.metrics_collector.timer("parsing_node"):
                    # ë¡œì»¬ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                    llm_client = LocalLLMClient(
                        endpoint=self.config.vllm_endpoint,
                        model=self.config.primary_model
                    )
                    
                    # íŒŒì‹± ì‹¤í–‰
                    parser = ProductionInputParser(llm_client)
                    parsed_result = await parser.parse_with_retry(
                        user_input=state["user_input"],
                        context=state["session_context"],
                        max_retries=3
                    )
                    
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    state["parsed_data"] = parsed_result
                    state["current_step"] = "parsing_completed"
                    state["metrics"]["parsing_confidence"] = parsed_result.confidence
                    
                    # í’ˆì§ˆ ê²€ì¦
                    if parsed_result.confidence < self.config.min_confidence_threshold:
                        state["quality_issues"].append("low_parsing_confidence")
                    
                    return state
                    
            except Exception as e:
                # ì—ëŸ¬ ì²˜ë¦¬
                state["errors"].append({
                    "step": "parsing",
                    "error": str(e),
                    "timestamp": datetime.utcnow()
                })
                state["current_step"] = "error_occurred"
                
                # ì—ëŸ¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                self.metrics_collector.increment("parsing_errors")
                self.logger.error(f"Parsing node error: {e}")
                
                return state
        
        return parsing_node
    
    def _define_workflow_edges(self, workflow: StateGraph):
        """
        ì›Œí¬í”Œë¡œìš° ì—£ì§€ ì •ì˜
        
        ë¶„ê¸° ë¡œì§:
        1. ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨ â†’ ì—ëŸ¬ í•¸ë“¤ëŸ¬
        2. íŒŒì‹± ì„±ê³µ â†’ ì •ê·œí™”
        3. ë²¡í„° ê²€ìƒ‰ í™œì„±í™” ì—¬ë¶€ì— ë”°ë¥¸ ë¶„ê¸°
        4. í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨ â†’ ì¬ì²˜ë¦¬ ë˜ëŠ” ì—ëŸ¬ ì²˜ë¦¬
        """
        # ìˆœì°¨ì  ì—£ì§€
        workflow.add_edge("input_validation", "parsing")
        workflow.add_edge("parsing", "normalization")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ - ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
        workflow.add_conditional_edges(
            "normalization",
            self._should_use_vector_search,
            {
                "use_vector": "vector_search",
                "skip_vector": "recommendation"
            }
        )
        
        workflow.add_edge("vector_search", "recommendation")
        workflow.add_edge("recommendation", "response_generation")
        workflow.add_edge("response_generation", "quality_check")
        
        # í’ˆì§ˆ ê²€ì¦ í›„ ë¶„ê¸°
        workflow.add_conditional_edges(
            "quality_check",
            self._quality_check_decision,
            {
                "approved": END,
                "needs_improvement": "parsing",  # ì¬ì²˜ë¦¬
                "critical_error": "error_handler"
            }
        )
        
        # ì—ëŸ¬ ì²˜ë¦¬
        workflow.add_edge("error_handler", END)
    
    def _should_use_vector_search(self, state: ProductionWorkflowState) -> str:
        """ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ ê²°ì •"""
        # ë²¡í„° ê²€ìƒ‰ í™œì„±í™” ì¡°ê±´
        conditions = [
            self.config.vector_search_enabled,
            state["parsed_data"].confidence > 0.7,
            len(state.get("session_context", {}).get("history", [])) > 0
        ]
        
        return "use_vector" if all(conditions) else "skip_vector"
    
    def _add_middleware(self, workflow: StateGraph) -> StateGraph:
        """
        ë¯¸ë“¤ì›¨ì–´ ì¶”ê°€
        
        í¬í•¨ ê¸°ëŠ¥:
        1. ìš”ì²­ ë¡œê¹…
        2. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        3. ì¸ì¦ ë° ê¶Œí•œ ê²€ì¦
        4. ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…
        """
        # ë¡œê¹… ë¯¸ë“¤ì›¨ì–´
        workflow = self._add_logging_middleware(workflow)
        
        # ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë¯¸ë“¤ì›¨ì–´
        workflow = self._add_metrics_middleware(workflow)
        
        # ë³´ì•ˆ ë¯¸ë“¤ì›¨ì–´
        workflow = self._add_security_middleware(workflow)
        
        return workflow
```

#### 2. Azure í†µí•© êµ¬í˜„ ê°€ì´ë“œ

```python
class AzureProductionDeployment:
    """
    Azure í™˜ê²½ ë°°í¬ ë° ê´€ë¦¬
    
    ê°œë°œíŒ€ êµ¬í˜„ ê°€ì´ë“œ:
    1. Infrastructure as Code (Terraform/ARM í…œí”Œë¦¿)
    2. CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    3. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹… ì„¤ì •
    4. ë³´ì•ˆ ë° ë„¤íŠ¸ì›Œí‚¹ êµ¬ì„±
    """
    
    def __init__(self, config: AzureDeploymentConfig):
        self.config = config
        self.resource_manager = AzureResourceManager(config)
        self.deployment_manager = DeploymentManager(config)
    
    async def deploy_production_stack(self) -> DeploymentResult:
        """
        ì „ì²´ ìŠ¤íƒ ë°°í¬
        
        ë°°í¬ ìˆœì„œ:
        1. Azure SQL Database
        2. Azure Container Registry
        3. AKS í´ëŸ¬ìŠ¤í„°
        4. vLLM ì„œë¹„ìŠ¤
        5. API Gateway
        6. ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ
        """
        deployment_steps = [
            ("database", self._deploy_azure_sql),
            ("container_registry", self._deploy_acr),
            ("kubernetes", self._deploy_aks),
            ("vllm_service", self._deploy_vllm),
            ("api_gateway", self._deploy_api_gateway),
            ("monitoring", self._deploy_monitoring)
        ]
        
        results = {}
        for step_name, deploy_func in deployment_steps:
            try:
                self.logger.info(f"Deploying {step_name}...")
                result = await deploy_func()
                results[step_name] = result
                self.logger.info(f"Successfully deployed {step_name}")
            except Exception as e:
                self.logger.error(f"Failed to deploy {step_name}: {e}")
                # ë¡¤ë°± ë¡œì§
                await self._rollback_deployment(step_name, results)
                raise
        
        return DeploymentResult(
            deployment_id=str(uuid.uuid4()),
            status="success",
            components=results,
            deployed_at=datetime.utcnow()
        )
    
    async def _deploy_azure_sql(self) -> Dict:
        """Azure SQL Database ë°°í¬"""
        sql_config = {
            "server_name": f"{self.config.project_name}-sql-server",
            "database_name": f"{self.config.project_name}-db",
            "tier": "Standard",
            "size": "S2",
            "backup_retention": 30,
            "geo_redundancy": True
        }
        
        # ARM í…œí”Œë¦¿ì„ í†µí•œ ë°°í¬
        template_path = "infrastructure/azure-sql-template.json"
        deployment_result = await self.resource_manager.deploy_template(
            template_path=template_path,
            parameters=sql_config
        )
        
        # ì—°ê²° ë¬¸ìì—´ ìƒì„± ë° Key Vault ì €ì¥
        connection_string = self._build_connection_string(sql_config)
        await self._store_secret("sql-connection-string", connection_string)
        
        return {
            "server_fqdn": deployment_result["outputs"]["serverFqdn"],
            "database_name": sql_config["database_name"],
            "connection_secret_name": "sql-connection-string"
        }
    
    async def _deploy_vllm(self) -> Dict:
        """vLLM ì„œë¹„ìŠ¤ ë°°í¬"""
        vllm_config = {
            "model_name": self.config.llm_model,
            "gpu_type": "Standard_NC6s_v3",  # V100 GPU
            "replica_count": 2,
            "max_model_len": 4096,
            "tensor_parallel_size": 1
        }
        
        # Kubernetes ë°°í¬ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
        k8s_manifest = self._generate_vllm_manifest(vllm_config)
        
        # AKSì— ë°°í¬
        deployment_result = await self.deployment_manager.deploy_to_aks(
            namespace="vllm",
            manifest=k8s_manifest
        )
        
        return {
            "service_endpoint": deployment_result["service_url"],
            "replica_count": vllm_config["replica_count"],
            "model_name": vllm_config["model_name"]
        }
```

---

## ğŸ“š ë¬¸ì„œ êµ¬ì¡° ë° ì°¸ì¡° ê°€ì´ë“œ

### 1. ë¬¸ì„œ ê³„ì¸µ êµ¬ì¡°

```
docs/
â”œâ”€â”€ PRODUCTION_MIGRATION_GUIDE.md     # ì´ ë¬¸ì„œ (ì „ì²´ ê°€ì´ë“œ)
â”œâ”€â”€ AI_RESEARCHER_GUIDE.md            # AI ì—°êµ¬ì› ì „ìš© ê°€ì´ë“œ
â”œâ”€â”€ DEVELOPER_GUIDE.md                # ê°œë°œíŒ€ ì „ìš© ê°€ì´ë“œ
â”œâ”€â”€ PERFORMANCE_EVALUATION_GUIDE.md   # ì„±ëŠ¥ í‰ê°€ ê°€ì´ë“œ
â””â”€â”€ DEPLOYMENT_GUIDE.md               # ë°°í¬ ë° ìš´ì˜ ê°€ì´ë“œ
```

### 2. ì½”ë“œ ì£¼ì„ ê·œì•½

```python
"""
ì½”ë“œ ì£¼ì„ ì‘ì„± ê·œì•½

=== AI ì—°êµ¬ì›ìš© ì£¼ì„ ===
- ëª©ì : ê° í•¨ìˆ˜/í´ë˜ìŠ¤ì˜ ì—­í• ê³¼ ì‹¤í—˜ í¬ì¸íŠ¸ ì„¤ëª…
- í˜•ì‹: ìƒì„¸í•œ ì„¤ëª… + ì‹¤í—˜ ê°€ì´ë“œ
- ì˜ˆì‹œ:

def normalize_term(self, term: str, category: str) -> Tuple[str, float]:
    '''
    ìš©ì–´ ì •ê·œí™” í•¨ìˆ˜
    
    AI ì—°êµ¬ì› ì°¸ê³ ì‚¬í•­:
    - ì´ í•¨ìˆ˜ëŠ” í˜„ì¬ LLM ê¸°ë°˜ìœ¼ë¡œ ë™ì‘
    - ë²¡í„° ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ êµì²´ ì‹œ ì„±ëŠ¥ í–¥ìƒ ì˜ˆìƒ
    - ì‹¤í—˜ í¬ì¸íŠ¸: ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ ë¹„êµ (notebooks/02_normalizer_experiment.ipynb ì°¸ì¡°)
    '''

=== ê°œë°œíŒ€ìš© ì£¼ì„ ===
- ëª©ì : ì•„í‚¤í…ì²˜ ì„¤ê³„ ì˜ë„ì™€ Production ê³ ë ¤ì‚¬í•­ ì„¤ëª…
- í˜•ì‹: ì„¤ê³„ ì›ì¹™ + êµ¬í˜„ ê°€ì´ë“œ
- ì˜ˆì‹œ:

class DatabaseManager:
    '''
    ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì €
    
    ê°œë°œíŒ€ ì°¸ê³ ì‚¬í•­:
    - í˜„ì¬: SQLite ì‚¬ìš© (í”„ë¡œí† íƒ€ì…)
    - Production: Azure SQL Database ì—°ë™ í•„ìš”
    - êµ¬í˜„ ì‹œ ê³ ë ¤ì‚¬í•­:
      1. ì—°ê²° í’€ë§ ë° íŠ¸ëœì­ì…˜ ê´€ë¦¬
      2. ì¿¼ë¦¬ ìµœì í™” ë° ì¸ë±ì‹±
      3. ì¥ì•  ë³µêµ¬ ë° ìë™ ì¬ì—°ê²°
    '''
"""
```

### 3. ì‹¤í—˜ ë…¸íŠ¸ë¶ í™œìš© ê°€ì´ë“œ

```python
"""
ì‹¤í—˜ ë…¸íŠ¸ë¶ í™œìš© ê°€ì´ë“œ

=== ë…¸íŠ¸ë¶ë³„ ì—­í•  ===
1. 01_parser_experiment.ipynb: íŒŒì‹± ë¡œì§ ì‹¤í—˜
2. 02_normalizer_experiment.ipynb: ì •ê·œí™” ë¡œì§ ì‹¤í—˜
3. 03_recommender_experiment.ipynb: ì¶”ì²œ ì—”ì§„ ì‹¤í—˜
4. 04_database_experiment.ipynb: ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤í—˜
5. 05_vector_embedding_experiment.ipynb: ë²¡í„° ì„ë² ë”© ì‹¤í—˜
6. 06_langgraph_experiment.ipynb: LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í—˜ (ì‹ ê·œ)
7. 07_performance_evaluation.ipynb: ì„±ëŠ¥ í‰ê°€ ì‹¤í—˜ (ì‹ ê·œ)

=== ë…¸íŠ¸ë¶ ì‘ì„± ì›ì¹™ ===
1. ìê¸°ì™„ê²°ì„±: ë…¸íŠ¸ë¶ ë‹¨ë…ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥
2. ì¬í˜„ì„±: ë™ì¼í•œ í™˜ê²½ì—ì„œ ë™ì¼í•œ ê²°ê³¼ ë³´ì¥
3. ë¬¸ì„œí™”: ì‹¤í—˜ ëª©ì , ë°©ë²•, ê²°ê³¼ ëª…í™•íˆ ê¸°ë¡
4. ë²„ì „ ê´€ë¦¬: ì‹¤í—˜ ê²°ê³¼ë¥¼ Gitìœ¼ë¡œ ì¶”ì 
"""
```

---

ì´ ê°€ì´ë“œëŠ” PMark3 í”„ë¡œí† íƒ€ì…ì„ Production í™˜ê²½ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì „í™˜í•˜ê¸° ìœ„í•œ ë¡œë“œë§µì„ ì œê³µí•©ë‹ˆë‹¤. AI ì—°êµ¬ì›ê³¼ ê°œë°œíŒ€ì´ ê°ìì˜ ì—­í• ì— ë§ê²Œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ êµ¬í˜„ ì‹œì—ëŠ” ë‹¨ê³„ì  ì ‘ê·¼ì„ í†µí•´ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•´ ë‚˜ê°€ì‹œê¸° ë°”ëë‹ˆë‹¤. 