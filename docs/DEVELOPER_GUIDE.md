# ê°œë°œíŒ€ì„ ìœ„í•œ PMark3 Production ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ì´ ë¬¸ì„œëŠ” ê°œë°œíŒ€ì´ PMark3 í”„ë¡œí† íƒ€ì…ì„ Production í™˜ê²½ìœ¼ë¡œ ì „í™˜í•˜ê¸° ìœ„í•œ ê¸°ìˆ ì  ê°€ì´ë“œì…ë‹ˆë‹¤. LangGraph ì•„í‚¤í…ì²˜, Azure ì¸í”„ë¼, ë¡œì»¬ LLM ì„œë¹™, ì„±ëŠ¥ ìµœì í™” ë“± Production ìˆ˜ì¤€ì˜ êµ¬í˜„ì— í•„ìš”í•œ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì „í™˜ ì „ëµ

### í˜„ì¬ í”„ë¡œí† íƒ€ì… vs Production ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "í˜„ì¬ í”„ë¡œí† íƒ€ì…"
        A1[FastAPI] --> B1[InputParser]
        B1 --> C1[Normalizer]
        C1 --> D1[Recommender]
        D1 --> E1[Response]
    end
    
    subgraph "Production ì•„í‚¤í…ì²˜"
        A2[API Gateway] --> B2[LangGraph Workflow]
        B2 --> C2[vLLM Cluster]
        B2 --> D2[Vector DB]
        B2 --> E2[Azure SQL]
        B2 --> F2[Monitoring]
    end
```

### ë§ˆì´ê·¸ë ˆì´ì…˜ ë‹¨ê³„

1. **Phase 1**: í˜„ì¬ ì½”ë“œ ê¸°ë°˜ ê°œì„  (ì„±ëŠ¥ ìµœì í™”, ì•ˆì •ì„± ê°•í™”)
2. **Phase 2**: LangGraph ë„ì… (ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„í™”)
3. **Phase 3**: ì¸í”„ë¼ ì „í™˜ (Azure ì´ì „, ë¡œì»¬ LLM ë„ì…)
4. **Phase 4**: ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ í†µí•©
5. **Phase 5**: ìš´ì˜ ìµœì í™” (ëª¨ë‹ˆí„°ë§, ìë™í™”)

## ğŸš€ LangGraph ê¸°ë°˜ ì•„í‚¤í…ì²˜ êµ¬í˜„

### 1. ì›Œí¬í”Œë¡œìš° ìƒíƒœ ê´€ë¦¬

```python
from typing import TypedDict, Optional, List, Dict, Any
from langgraph.graph import StateGraph, END
import asyncio
import time
import logging

class ProductionWorkflowState(TypedDict):
    """
    Production ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì •ì˜
    
    ì„¤ê³„ ì›ì¹™:
    1. ë¶ˆë³€ì„±: ìƒíƒœ ë³€ê²½ì€ ìƒˆë¡œìš´ ìƒíƒœ ê°ì²´ ìƒì„±
    2. ì§ë ¬í™” ê°€ëŠ¥: Redis/Azure Cache ì €ì¥ ì§€ì›
    3. íƒ€ì… ì•ˆì „ì„±: TypedDictë¡œ íƒ€ì… ì²´í¬
    4. í™•ì¥ì„±: ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€ ìš©ì´
    """
    # ì…ë ¥ ë°ì´í„°
    user_input: str
    session_id: str
    request_id: str
    
    # ì²˜ë¦¬ ë‹¨ê³„ë³„ ê²°ê³¼
    parsed_data: Optional[Dict[str, Any]]
    normalized_data: Optional[Dict[str, Any]]
    vector_search_results: Optional[List[Dict]]
    recommendations: Optional[List[Dict]]
    final_response: Optional[str]
    
    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´
    session_context: Dict[str, Any]
    conversation_history: List[Dict[str, Any]]
    
    # ë©”íƒ€ë°ì´í„°
    current_step: str
    processing_start_time: float
    step_timings: Dict[str, float]
    
    # ì—ëŸ¬ ë° í’ˆì§ˆ ê´€ë¦¬
    errors: List[Dict[str, Any]]
    warnings: List[str]
    quality_scores: Dict[str, float]
    
    # ì„¤ì •
    config: Dict[str, Any]


class ProductionWorkflowBuilder:
    """
    Productionìš© LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë”
    
    í•µì‹¬ ì„¤ê³„ íŒ¨í„´:
    1. ì±…ì„ ë¶„ë¦¬: ê° ë…¸ë“œëŠ” ë‹¨ì¼ ì±…ì„
    2. ì—ëŸ¬ ê²½ê³„: ë…¸ë“œë³„ ì—ëŸ¬ ì²˜ë¦¬
    3. ê´€ì¸¡ ê°€ëŠ¥ì„±: ëª¨ë“  ë‹¨ê³„ì—ì„œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    4. ë³µì›ë ¥: ì¥ì•  ì‹œ ìë™ ë³µêµ¬
    """
    
    def __init__(self, config: ProductionConfig):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.metrics_collector = MetricsCollector()
        self.state_manager = StateManager(config.redis_url)
        
    def build_workflow(self) -> CompiledGraph:
        """
        Production ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        
        ì›Œí¬í”Œë¡œìš° êµ¬ì¡°:
        1. ì…ë ¥ ê²€ì¦ â†’ íŒŒì‹± â†’ ì •ê·œí™”
        2. ì¡°ê±´ë¶€ ë¶„ê¸°: ë²¡í„° ê²€ìƒ‰ ì—¬ë¶€
        3. ì¶”ì²œ ìƒì„± â†’ ì‘ë‹µ ìƒì„±
        4. í’ˆì§ˆ ê²€ì¦ â†’ ìµœì¢… ì‘ë‹µ
        
        ì—ëŸ¬ ì²˜ë¦¬:
        - ê° ë…¸ë“œì—ì„œ ì˜ˆì™¸ ë°œìƒ ì‹œ ì—ëŸ¬ ë…¸ë“œë¡œ ë¶„ê¸°
        - ì¬ì‹œë„ ê°€ëŠ¥í•œ ì—ëŸ¬ëŠ” ìë™ ì¬ì‹œë„
        - ì¹˜ëª…ì  ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì—ëŸ¬ ì‘ë‹µ
        """
        workflow = StateGraph(ProductionWorkflowState)
        
        # ë…¸ë“œ ë“±ë¡
        workflow.add_node("validate_input", self._validate_input_node)
        workflow.add_node("parse_input", self._parse_input_node)
        workflow.add_node("normalize_terms", self._normalize_terms_node)
        workflow.add_node("vector_search", self._vector_search_node)
        workflow.add_node("generate_recommendations", self._generate_recommendations_node)
        workflow.add_node("generate_response", self._generate_response_node)
        workflow.add_node("quality_check", self._quality_check_node)
        workflow.add_node("handle_error", self._handle_error_node)
        
        # ì—£ì§€ ì •ì˜
        self._define_workflow_edges(workflow)
        
        # ì‹œì‘ì  ì„¤ì •
        workflow.set_entry_point("validate_input")
        
        return workflow.compile()
    
    async def _validate_input_node(self, state: ProductionWorkflowState) -> ProductionWorkflowState:
        """
        ì…ë ¥ ê²€ì¦ ë…¸ë“œ
        
        ê²€ì¦ í•­ëª©:
        1. ì…ë ¥ ê¸¸ì´ ì œí•œ (1-1000ì)
        2. ê¸ˆì§€ ë¬¸ì í•„í„°ë§
        3. ì¸ì½”ë”© ê²€ì¦ (UTF-8)
        4. ì„¸ì…˜ ìœ íš¨ì„± ê²€ì‚¬
        
        ì—ëŸ¬ ì²˜ë¦¬:
        - ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ìƒíƒœë¡œ ì „í™˜
        - ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€ ì œê³µ
        """
        start_time = time.time()
        
        try:
            # ê¸°ë³¸ ì…ë ¥ ê²€ì¦
            user_input = state["user_input"].strip()
            
            if not user_input:
                raise ValueError("Empty input provided")
            
            if len(user_input) > 1000:
                raise ValueError("Input too long (max 1000 characters)")
            
            if len(user_input) < 3:
                raise ValueError("Input too short (min 3 characters)")
            
            # ì„¸ì…˜ ìœ íš¨ì„± ê²€ì‚¬
            session_id = state["session_id"]
            if not await self.state_manager.is_session_valid(session_id):
                raise ValueError(f"Invalid session: {session_id}")
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state["current_step"] = "input_validated"
            state["step_timings"]["validation"] = time.time() - start_time
            
            self.logger.info(f"Input validation passed for session {session_id}")
            return state
            
        except Exception as e:
            # ì—ëŸ¬ ìƒíƒœë¡œ ì „í™˜
            state["current_step"] = "validation_error"
            state["errors"].append({
                "step": "validation",
                "error": str(e),
                "timestamp": time.time()
            })
            
            self.logger.error(f"Input validation failed: {e}")
            return state
    
    async def _parse_input_node(self, state: ProductionWorkflowState) -> ProductionWorkflowState:
        """
        ì…ë ¥ íŒŒì‹± ë…¸ë“œ
        
        ì²˜ë¦¬ ê³¼ì •:
        1. ë¡œì»¬ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        2. ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
        3. ì‹œë‚˜ë¦¬ì˜¤ ë¶„ê¸° (S1/S2/S3)
        4. LLM í˜¸ì¶œ ë° íŒŒì‹±
        5. ê²°ê³¼ ê²€ì¦ ë° í›„ì²˜ë¦¬
        
        ì„±ëŠ¥ ìµœì í™”:
        - ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
        - ê²°ê³¼ ìºì‹±
        - íƒ€ì„ì•„ì›ƒ ê´€ë¦¬
        """
        start_time = time.time()
        
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            llm_client = await self._get_llm_client()
            
            # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ
            session_context = await self.state_manager.get_session_context(
                state["session_id"]
            )
            
            # íŒŒì„œ ì´ˆê¸°í™”
            parser = ProductionInputParser(
                llm_client=llm_client,
                config=self.config.parser_config
            )
            
            # íŒŒì‹± ì‹¤í–‰
            parsed_result = await parser.parse_with_context(
                user_input=state["user_input"],
                session_context=session_context,
                conversation_history=state["conversation_history"]
            )
            
            # ê²°ê³¼ ê²€ì¦
            if parsed_result.confidence < self.config.min_parsing_confidence:
                state["warnings"].append(
                    f"Low parsing confidence: {parsed_result.confidence}"
                )
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state["parsed_data"] = parsed_result.to_dict()
            state["current_step"] = "parsing_completed"
            state["step_timings"]["parsing"] = time.time() - start_time
            state["quality_scores"]["parsing"] = parsed_result.confidence
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            await self.metrics_collector.record_parsing_metrics(
                session_id=state["session_id"],
                input_length=len(state["user_input"]),
                parsing_time=time.time() - start_time,
                confidence=parsed_result.confidence,
                scenario=parsed_result.scenario
            )
            
            return state
            
        except Exception as e:
            # ì—ëŸ¬ ì²˜ë¦¬
            state["current_step"] = "parsing_error"
            state["errors"].append({
                "step": "parsing",
                "error": str(e),
                "timestamp": time.time(),
                "input_sample": state["user_input"][:100]  # ë””ë²„ê¹…ìš©
            })
            
            # ì—ëŸ¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            await self.metrics_collector.record_error(
                step="parsing",
                error_type=type(e).__name__,
                session_id=state["session_id"]
            )
            
            return state
    
    async def _normalize_terms_node(self, state: ProductionWorkflowState) -> ProductionWorkflowState:
        """
        ìš©ì–´ ì •ê·œí™” ë…¸ë“œ
        
        ì •ê·œí™” ì „ëµ:
        1. ìºì‹œ ìš°ì„  í™•ì¸
        2. ë²¡í„° ê¸°ë°˜ ì •ê·œí™” (í™œì„±í™”ëœ ê²½ìš°)
        3. LLM ê¸°ë°˜ í´ë°± ì •ê·œí™”
        4. ê²°ê³¼ ìºì‹± ë° í’ˆì§ˆ í‰ê°€
        
        ë²¡í„° ê¸°ë°˜ ì •ê·œí™” ë¡œì§:
        - ì…ë ¥ ìš©ì–´ ì„ë² ë”© ìƒì„±
        - ë²¡í„° DBì—ì„œ ìœ ì‚¬ í‘œì¤€ ìš©ì–´ ê²€ìƒ‰
        - ìœ ì‚¬ë„ ì„ê³„ê°’ ê¸°ë°˜ ì •ê·œí™”
        """
        start_time = time.time()
        
        try:
            parsed_data = state["parsed_data"]
            if not parsed_data:
                raise ValueError("No parsed data available for normalization")
            
            # ì •ê·œí™” ì—”ì§„ ì´ˆê¸°í™”
            if self.config.vector_normalization_enabled:
                normalizer = VectorBasedNormalizer(
                    vector_db=await self._get_vector_db(),
                    embedding_service=await self._get_embedding_service()
                )
            else:
                normalizer = LLMNormalizer(
                    llm_client=await self._get_llm_client()
                )
            
            # ê° í•„ë“œë³„ ì •ê·œí™”
            normalized_results = {}
            categories = ["location", "equipment_type", "status_code", "priority"]
            
            for category in categories:
                if parsed_data.get(category):
                    term = parsed_data[category]
                    
                    # ì •ê·œí™” ì‹¤í–‰
                    normalized_term, confidence = await normalizer.normalize_term(
                        term=term,
                        category=category
                    )
                    
                    normalized_results[category] = {
                        "original": term,
                        "normalized": normalized_term,
                        "confidence": confidence
                    }
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state["normalized_data"] = normalized_results
            state["current_step"] = "normalization_completed"
            state["step_timings"]["normalization"] = time.time() - start_time
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            avg_confidence = sum(
                result["confidence"] for result in normalized_results.values()
            ) / len(normalized_results) if normalized_results else 0
            
            state["quality_scores"]["normalization"] = avg_confidence
            
            return state
            
        except Exception as e:
            state["current_step"] = "normalization_error"
            state["errors"].append({
                "step": "normalization",
                "error": str(e),
                "timestamp": time.time()
            })
            return state
    
    async def _vector_search_node(self, state: ProductionWorkflowState) -> ProductionWorkflowState:
        """
        ë²¡í„° ê²€ìƒ‰ ë…¸ë“œ
        
        ê²€ìƒ‰ ê³¼ì •:
        1. ì •ê·œí™”ëœ ë°ì´í„°ë¥¼ ì¿¼ë¦¬ë¡œ ë³€í™˜
        2. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        3. ë²¡í„° DBì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
        4. ê²°ê³¼ í•„í„°ë§ ë° ë­í‚¹
        5. ê¸°ì¡´ DB ê²€ìƒ‰ ê²°ê³¼ì™€ ë³‘í•©
        
        ì„±ëŠ¥ ìµœì í™”:
        - ë³‘ë ¬ ê²€ìƒ‰ (ë‹¤ì¤‘ ì»¬ë ‰ì…˜)
        - ê²°ê³¼ ìºì‹±
        - ì ì‘ì  ì„ê³„ê°’ ì¡°ì •
        """
        start_time = time.time()
        
        try:
            normalized_data = state["normalized_data"]
            if not normalized_data:
                # ë²¡í„° ê²€ìƒ‰ ìƒëµ
                state["vector_search_results"] = []
                state["current_step"] = "vector_search_skipped"
                return state
            
            # ë²¡í„° DB ë° ì„ë² ë”© ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
            vector_db = await self._get_vector_db()
            embedding_service = await self._get_embedding_service()
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
            search_query = self._build_search_query(normalized_data)
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = await embedding_service.encode_batch(
                texts=[search_query],
                model_name=self.config.search_embedding_model
            )
            
            # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
            search_tasks = []
            
            # ê° ì»¬ë ‰ì…˜ë³„ ë³‘ë ¬ ê²€ìƒ‰
            collections = ["equipment_docs", "procedure_docs", "location_docs"]
            for collection in collections:
                task = vector_db.search(
                    embedding=query_embedding[0],
                    collection=collection,
                    top_k=10,
                    threshold=self.config.vector_search_threshold
                )
                search_tasks.append(task)
            
            # ë³‘ë ¬ ì‹¤í–‰
            search_results = await asyncio.gather(*search_tasks)
            
            # ê²°ê³¼ ë³‘í•© ë° ë­í‚¹
            merged_results = self._merge_and_rank_vector_results(search_results)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state["vector_search_results"] = merged_results
            state["current_step"] = "vector_search_completed"
            state["step_timings"]["vector_search"] = time.time() - start_time
            
            # ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€
            search_quality = self._evaluate_search_quality(merged_results)
            state["quality_scores"]["vector_search"] = search_quality
            
            return state
            
        except Exception as e:
            state["current_step"] = "vector_search_error"
            state["errors"].append({
                "step": "vector_search",
                "error": str(e),
                "timestamp": time.time()
            })
            return state
    
    def _define_workflow_edges(self, workflow: StateGraph):
        """
        ì›Œí¬í”Œë¡œìš° ì—£ì§€ ì •ì˜
        
        ë¶„ê¸° ë¡œì§:
        1. ì…ë ¥ ê²€ì¦ ì‹¤íŒ¨ â†’ ì—ëŸ¬ ì²˜ë¦¬
        2. íŒŒì‹± ì„±ê³µ â†’ ì •ê·œí™”
        3. ë²¡í„° ê²€ìƒ‰ í™œì„±í™” ì—¬ë¶€ ë¶„ê¸°
        4. í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ì¬ì²˜ë¦¬ ë¶„ê¸°
        """
        # ìˆœì°¨ ì—£ì§€
        workflow.add_conditional_edges(
            "validate_input",
            self._validation_router,
            {
                "continue": "parse_input",
                "error": "handle_error"
            }
        )
        
        workflow.add_conditional_edges(
            "parse_input",
            self._parsing_router,
            {
                "continue": "normalize_terms",
                "error": "handle_error"
            }
        )
        
        workflow.add_conditional_edges(
            "normalize_terms",
            self._normalization_router,
            {
                "use_vector": "vector_search",
                "skip_vector": "generate_recommendations",
                "error": "handle_error"
            }
        )
        
        workflow.add_edge("vector_search", "generate_recommendations")
        workflow.add_edge("generate_recommendations", "generate_response")
        workflow.add_edge("generate_response", "quality_check")
        
        workflow.add_conditional_edges(
            "quality_check",
            self._quality_router,
            {
                "approved": END,
                "retry": "parse_input",
                "error": "handle_error"
            }
        )
        
        workflow.add_edge("handle_error", END)
    
    def _validation_router(self, state: ProductionWorkflowState) -> str:
        """ì…ë ¥ ê²€ì¦ ê²°ê³¼ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
        if state["current_step"] == "validation_error":
            return "error"
        return "continue"
    
    def _parsing_router(self, state: ProductionWorkflowState) -> str:
        """íŒŒì‹± ê²°ê³¼ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
        if state["current_step"] == "parsing_error":
            return "error"
        return "continue"
    
    def _normalization_router(self, state: ProductionWorkflowState) -> str:
        """ì •ê·œí™” ê²°ê³¼ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
        if state["current_step"] == "normalization_error":
            return "error"
        
        # ë²¡í„° ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        conditions = [
            self.config.vector_search_enabled,
            state["quality_scores"].get("normalization", 0) > 0.7,
            len(state.get("conversation_history", [])) > 0
        ]
        
        if all(conditions):
            return "use_vector"
        else:
            return "skip_vector"
    
    def _quality_router(self, state: ProductionWorkflowState) -> str:
        """í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ì— ë”°ë¥¸ ë¼ìš°íŒ…"""
        overall_quality = sum(state["quality_scores"].values()) / len(state["quality_scores"])
        
        if overall_quality < self.config.min_quality_threshold:
            retry_count = len([e for e in state["errors"] if "retry" in e.get("tags", [])])
            if retry_count < self.config.max_retry_count:
                return "retry"
            else:
                return "error"
        
        return "approved"


class ProductionInputParser:
    """
    Productionìš© ì…ë ¥ íŒŒì„œ
    
    ê°œì„ ì‚¬í•­:
    1. ë¡œì»¬ LLM í†µí•©
    2. ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
    3. ìºì‹± ë° ì¬ì‹œë„ ë¡œì§
    4. ìƒì„¸í•œ ì—ëŸ¬ ì²˜ë¦¬
    """
    
    def __init__(self, llm_client: LocalLLMClient, config: ParserConfig):
        self.llm_client = llm_client
        self.config = config
        self.cache = ParsingCache(maxsize=1000, ttl=3600)
        
    async def parse_with_context(self, 
                               user_input: str, 
                               session_context: Dict,
                               conversation_history: List[Dict]) -> ParsedInput:
        """
        ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•œ ì…ë ¥ íŒŒì‹±
        
        ìµœì í™” ê¸°ë²•:
        1. ìºì‹œ ìš°ì„  í™•ì¸
        2. ë°°ì¹˜ ì²˜ë¦¬ (ë™ì¼ ì„¸ì…˜ ë‚´ ìš”ì²­)
        3. ì ì‘ì  í”„ë¡¬í”„íŠ¸ ì„ íƒ
        4. ê²°ê³¼ í›„ì²˜ë¦¬ ë° ê²€ì¦
        """
        # ìºì‹œ í™•ì¸
        cache_key = self._generate_cache_key(user_input, session_context)
        cached_result = self.cache.get(cache_key)
        if cached_result:
            return cached_result
        
        # ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
        scenario = self._detect_scenario(user_input, conversation_history)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._create_contextual_prompt(
            user_input=user_input,
            scenario=scenario,
            session_context=session_context,
            conversation_history=conversation_history
        )
        
        # LLM í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        max_retries = 3
        for attempt in range(max_retries):
            try:
                response = await self.llm_client.generate(
                    prompt=prompt,
                    max_tokens=512,
                    temperature=0.1,
                    timeout=30
                )
                
                # ì‘ë‹µ íŒŒì‹±
                parsed_result = self._parse_llm_response(response)
                
                # ê²°ê³¼ ê²€ì¦
                if self._validate_parsing_result(parsed_result):
                    # ìºì‹œ ì €ì¥
                    self.cache.set(cache_key, parsed_result)
                    return parsed_result
                else:
                    raise ValueError("Parsing result validation failed")
                    
            except Exception as e:
                if attempt == max_retries - 1:
                    raise
                await asyncio.sleep(0.5 * (attempt + 1))  # ë°±ì˜¤í”„
        
        raise RuntimeError("Parsing failed after all retries")
```

## ğŸ”§ Azure ì¸í”„ë¼ êµ¬ì„±

### 1. Infrastructure as Code

```python
# infrastructure/azure_deployment.py
class AzureInfrastructureManager:
    """
    Azure ì¸í”„ë¼ ê´€ë¦¬
    
    êµ¬ì„± ìš”ì†Œ:
    1. Azure SQL Database (ê³ ê°€ìš©ì„±)
    2. Azure Kubernetes Service (AKS)
    3. Azure Container Registry (ACR)
    4. Azure Key Vault (ì‹œí¬ë¦¿ ê´€ë¦¬)
    5. Azure Monitor (ëª¨ë‹ˆí„°ë§)
    6. Azure Application Gateway (ë¡œë“œ ë°¸ëŸ°ì‹±)
    """
    
    def __init__(self, config: AzureConfig):
        self.config = config
        self.resource_client = ResourceManagementClient(
            credential=DefaultAzureCredential(),
            subscription_id=config.subscription_id
        )
        
    async def deploy_full_stack(self) -> DeploymentResult:
        """
        ì „ì²´ ìŠ¤íƒ ë°°í¬
        
        ë°°í¬ ìˆœì„œ:
        1. ë¦¬ì†ŒìŠ¤ ê·¸ë£¹ ìƒì„±
        2. ë„¤íŠ¸ì›Œí‚¹ (VNet, ì„œë¸Œë„·)
        3. Key Vault (ì‹œí¬ë¦¿ ì €ì¥ì†Œ)
        4. Azure SQL Database
        5. Container Registry
        6. AKS í´ëŸ¬ìŠ¤í„°
        7. Application Gateway
        8. ëª¨ë‹ˆí„°ë§ ì„¤ì •
        """
        deployment_steps = [
            self._create_resource_group,
            self._setup_networking,
            self._create_key_vault,
            self._deploy_sql_database,
            self._create_container_registry,
            self._deploy_aks_cluster,
            self._setup_application_gateway,
            self._configure_monitoring
        ]
        
        results = {}
        for i, step_func in enumerate(deployment_steps):
            step_name = step_func.__name__
            try:
                self.logger.info(f"Executing step {i+1}: {step_name}")
                result = await step_func()
                results[step_name] = result
                self.logger.info(f"Completed step {i+1}: {step_name}")
            except Exception as e:
                self.logger.error(f"Failed step {i+1}: {step_name} - {e}")
                # ë¡¤ë°± ë¡œì§
                await self._rollback_deployment(results)
                raise
        
        return DeploymentResult(
            status="success",
            components=results,
            endpoints=self._extract_endpoints(results)
        )
    
    async def _deploy_sql_database(self) -> Dict[str, Any]:
        """
        Azure SQL Database ë°°í¬
        
        ì„¤ì •:
        - ê³ ê°€ìš©ì„± (Zone Redundant)
        - ìë™ ë°±ì—… (PITR 35ì¼)
        - ì•”í˜¸í™” (TDE)
        - ë°©í™”ë²½ ê·œì¹™
        - ì—°ê²° í’€ë§
        """
        sql_config = {
            "server_name": f"{self.config.project_name}-sql-{self.config.environment}",
            "database_name": f"{self.config.project_name}-db",
            "location": self.config.primary_region,
            "tier": "Standard",
            "service_objective": "S2",
            "max_size_bytes": 268435456000,  # 250GB
            "zone_redundant": True,
            "backup_retention_days": 35,
            "geo_redundant_backup": True
        }
        
        # ARM í…œí”Œë¦¿ ì‚¬ìš©
        template = self._load_arm_template("sql-database-template.json")
        
        deployment_result = await self.resource_client.deployments.begin_create_or_update(
            resource_group_name=self.config.resource_group_name,
            deployment_name=f"sql-deployment-{int(time.time())}",
            parameters={
                "template": template,
                "parameters": sql_config
            }
        ).result()
        
        # ì—°ê²° ë¬¸ìì—´ ìƒì„± ë° Key Vault ì €ì¥
        connection_string = self._build_sql_connection_string(sql_config)
        await self._store_secret("sql-connection-string", connection_string)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        await self._initialize_database(connection_string)
        
        return {
            "server_fqdn": f"{sql_config['server_name']}.database.windows.net",
            "database_name": sql_config["database_name"],
            "connection_secret": "sql-connection-string"
        }
    
    async def _deploy_aks_cluster(self) -> Dict[str, Any]:
        """
        AKS í´ëŸ¬ìŠ¤í„° ë°°í¬
        
        ì„¤ì •:
        - ë‹¤ì¤‘ ë…¸ë“œ í’€ (ì‹œìŠ¤í…œ/ì‚¬ìš©ì)
        - GPU ë…¸ë“œ (vLLMìš©)
        - ìë™ ìŠ¤ì¼€ì¼ë§
        - Azure AD í†µí•©
        - ë„¤íŠ¸ì›Œí¬ ì •ì±…
        """
        aks_config = {
            "cluster_name": f"{self.config.project_name}-aks-{self.config.environment}",
            "location": self.config.primary_region,
            "kubernetes_version": "1.28.0",
            "dns_prefix": f"{self.config.project_name}-aks",
            
            # ì‹œìŠ¤í…œ ë…¸ë“œ í’€
            "system_node_pool": {
                "name": "systempool",
                "node_count": 3,
                "vm_size": "Standard_D4s_v3",
                "os_disk_size": 100,
                "auto_scaling": True,
                "min_count": 1,
                "max_count": 5
            },
            
            # GPU ë…¸ë“œ í’€ (vLLMìš©)
            "gpu_node_pool": {
                "name": "gpupool",
                "node_count": 2,
                "vm_size": "Standard_NC6s_v3",  # Tesla V100
                "os_disk_size": 200,
                "auto_scaling": True,
                "min_count": 1,
                "max_count": 4,
                "node_taints": ["gpu=true:NoSchedule"]
            }
        }
        
        # AKS í´ëŸ¬ìŠ¤í„° ìƒì„±
        cluster_result = await self._create_aks_cluster(aks_config)
        
        # kubectl ì„¤ì •
        await self._configure_kubectl(aks_config["cluster_name"])
        
        # í•„ìˆ˜ ì• ë“œì˜¨ ì„¤ì¹˜
        await self._install_aks_addons()
        
        return {
            "cluster_name": aks_config["cluster_name"],
            "cluster_fqdn": cluster_result["fqdn"],
            "node_pools": ["systempool", "gpupool"]
        }


class vLLMDeploymentManager:
    """
    vLLM ì„œë¹„ìŠ¤ ë°°í¬ ê´€ë¦¬
    
    ë°°í¬ ì „ëµ:
    1. ëª¨ë¸ë³„ ì „ìš© ë…¸ë“œ í’€
    2. GPU ë¦¬ì†ŒìŠ¤ ê²©ë¦¬
    3. ìë™ ìŠ¤ì¼€ì¼ë§
    4. ë¡¤ë§ ì—…ë°ì´íŠ¸
    5. í—¬ìŠ¤ì²´í¬ ë° ëª¨ë‹ˆí„°ë§
    """
    
    def __init__(self, k8s_client, config: vLLMConfig):
        self.k8s_client = k8s_client
        self.config = config
        
    async def deploy_vllm_service(self, model_name: str) -> Dict[str, Any]:
        """
        vLLM ì„œë¹„ìŠ¤ ë°°í¬
        
        ë°°í¬ êµ¬ì„±:
        1. Deployment (vLLM ì„œë²„)
        2. Service (í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ ì ‘ê·¼)
        3. Ingress (ì™¸ë¶€ ì ‘ê·¼)
        4. HPA (ìˆ˜í‰ í™•ì¥)
        5. PDB (Pod ì¤‘ë‹¨ ì˜ˆì‚°)
        """
        model_config = self.config.models[model_name]
        
        # Deployment ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
        deployment_manifest = self._create_vllm_deployment(model_name, model_config)
        
        # Service ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
        service_manifest = self._create_vllm_service(model_name)
        
        # Ingress ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
        ingress_manifest = self._create_vllm_ingress(model_name)
        
        # HPA ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±
        hpa_manifest = self._create_vllm_hpa(model_name)
        
        # ìˆœì°¨ ë°°í¬
        await self._apply_manifest(deployment_manifest)
        await self._apply_manifest(service_manifest)
        await self._apply_manifest(ingress_manifest)
        await self._apply_manifest(hpa_manifest)
        
        # ë°°í¬ ìƒíƒœ í™•ì¸
        await self._wait_for_deployment_ready(model_name)
        
        # í—¬ìŠ¤ì²´í¬
        service_url = await self._get_service_url(model_name)
        await self._verify_vllm_health(service_url)
        
        return {
            "model_name": model_name,
            "service_url": service_url,
            "replicas": model_config["min_replicas"],
            "gpu_type": model_config["gpu_type"]
        }
    
    def _create_vllm_deployment(self, model_name: str, model_config: Dict) -> Dict:
        """vLLM Deployment ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ìƒì„±"""
        return {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": f"vllm-{model_name}",
                "namespace": "vllm",
                "labels": {
                    "app": "vllm",
                    "model": model_name
                }
            },
            "spec": {
                "replicas": model_config["min_replicas"],
                "selector": {
                    "matchLabels": {
                        "app": "vllm",
                        "model": model_name
                    }
                },
                "template": {
                    "metadata": {
                        "labels": {
                            "app": "vllm",
                            "model": model_name
                        }
                    },
                    "spec": {
                        "nodeSelector": {
                            "gpu": "true"
                        },
                        "tolerations": [{
                            "key": "gpu",
                            "operator": "Equal",
                            "value": "true",
                            "effect": "NoSchedule"
                        }],
                        "containers": [{
                            "name": "vllm",
                            "image": f"vllm/vllm-openai:{self.config.vllm_version}",
                            "args": [
                                "--model", model_config["model_path"],
                                "--tensor-parallel-size", str(model_config["tensor_parallel_size"]),
                                "--max-model-len", str(model_config["max_model_len"]),
                                "--gpu-memory-utilization", str(model_config["gpu_memory_utilization"]),
                                "--host", "0.0.0.0",
                                "--port", "8000"
                            ],
                            "ports": [{"containerPort": 8000}],
                            "resources": {
                                "requests": {
                                    "nvidia.com/gpu": model_config["tensor_parallel_size"],
                                    "memory": "16Gi",
                                    "cpu": "4"
                                },
                                "limits": {
                                    "nvidia.com/gpu": model_config["tensor_parallel_size"],
                                    "memory": "32Gi",
                                    "cpu": "8"
                                }
                            },
                            "livenessProbe": {
                                "httpGet": {
                                    "path": "/health",
                                    "port": 8000
                                },
                                "initialDelaySeconds": 60,
                                "periodSeconds": 30
                            },
                            "readinessProbe": {
                                "httpGet": {
                                    "path": "/v1/models",
                                    "port": 8000
                                },
                                "initialDelaySeconds": 30,
                                "periodSeconds": 10
                            }
                        }]
                    }
                }
            }
        }
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° í‰ê°€ ì‹œìŠ¤í…œ

### 1. ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
class ProductionMonitoringSystem:
    """
    Production ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
    
    ëª¨ë‹ˆí„°ë§ ì˜ì—­:
    1. API ì„±ëŠ¥ (ì‘ë‹µ ì‹œê°„, ì²˜ë¦¬ëŸ‰, ì—ëŸ¬ìœ¨)
    2. LLM ì„±ëŠ¥ (í† í°/ì´ˆ, ì§€ì—°ì‹œê°„, GPU ì‚¬ìš©ë¥ )
    3. ë²¡í„° DB ì„±ëŠ¥ (ê²€ìƒ‰ ì†ë„, ì •í™•ë„)
    4. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ (CPU, ë©”ëª¨ë¦¬, ë„¤íŠ¸ì›Œí¬)
    5. ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ (ì‚¬ìš©ì ë§Œì¡±ë„, ì¶”ì²œ ì •í™•ë„)
    """
    
    def __init__(self, config: MonitoringConfig):
        self.config = config
        self.metrics_client = AzureMetricsClient()
        self.alerting_client = AlertingClient()
        self.performance_db = PerformanceDatabase()
        
    async def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì‹œì‘"""
        monitoring_tasks = [
            self._monitor_api_performance(),
            self._monitor_llm_performance(),
            self._monitor_vector_db_performance(),
            self._monitor_system_resources(),
            self._monitor_business_metrics()
        ]
        
        await asyncio.gather(*monitoring_tasks)
    
    async def _monitor_api_performance(self):
        """API ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        while True:
            try:
                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                api_metrics = await self._collect_api_metrics()
                
                # ì„ê³„ê°’ í™•ì¸
                alerts = self._check_api_thresholds(api_metrics)
                
                # ì•Œë¦¼ ë°œì†¡
                for alert in alerts:
                    await self.alerting_client.send_alert(alert)
                
                # ë©”íŠ¸ë¦­ ì €ì¥
                await self.performance_db.store_metrics("api", api_metrics)
                
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ìˆ˜ì§‘
                
            except Exception as e:
                logger.error(f"API monitoring error: {e}")
                await asyncio.sleep(60)
    
    async def _collect_api_metrics(self) -> Dict[str, float]:
        """API ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        metrics = {}
        
        # ì‘ë‹µ ì‹œê°„ í†µê³„
        response_times = await self.metrics_client.get_response_times(
            time_range="5m"
        )
        metrics["avg_response_time"] = np.mean(response_times)
        metrics["p95_response_time"] = np.percentile(response_times, 95)
        metrics["p99_response_time"] = np.percentile(response_times, 99)
        
        # ì²˜ë¦¬ëŸ‰
        request_count = await self.metrics_client.get_request_count(
            time_range="1m"
        )
        metrics["requests_per_minute"] = request_count
        
        # ì—ëŸ¬ìœ¨
        error_count = await self.metrics_client.get_error_count(
            time_range="5m"
        )
        total_requests = await self.metrics_client.get_request_count(
            time_range="5m"
        )
        metrics["error_rate"] = error_count / total_requests if total_requests > 0 else 0
        
        return metrics
    
    def _check_api_thresholds(self, metrics: Dict[str, float]) -> List[Alert]:
        """API ì„ê³„ê°’ í™•ì¸"""
        alerts = []
        
        # ì‘ë‹µ ì‹œê°„ ì•Œë¦¼
        if metrics["p95_response_time"] > self.config.max_p95_response_time:
            alerts.append(Alert(
                severity="warning",
                message=f"High P95 response time: {metrics['p95_response_time']:.2f}s",
                metric="p95_response_time",
                value=metrics["p95_response_time"],
                threshold=self.config.max_p95_response_time
            ))
        
        # ì—ëŸ¬ìœ¨ ì•Œë¦¼
        if metrics["error_rate"] > self.config.max_error_rate:
            alerts.append(Alert(
                severity="critical",
                message=f"High error rate: {metrics['error_rate']:.2%}",
                metric="error_rate",
                value=metrics["error_rate"],
                threshold=self.config.max_error_rate
            ))
        
        return alerts


class PerformanceEvaluationSuite:
    """
    ì„±ëŠ¥ í‰ê°€ ë„êµ¬ ëª¨ìŒ
    
    í‰ê°€ ì˜ì—­:
    1. íŒŒì‹± ì •í™•ë„ ë° ì†ë„
    2. ì •ê·œí™” ì¼ê´€ì„± ë° ì„±ëŠ¥
    3. ì¶”ì²œ í’ˆì§ˆ ë° ë‹¤ì–‘ì„±
    4. ë²¡í„° ê²€ìƒ‰ ì •í™•ë„ ë° ì†ë„
    5. ì „ì²´ ì‹œìŠ¤í…œ ì²˜ë¦¬ëŸ‰
    """
    
    def __init__(self, config: EvaluationConfig):
        self.config = config
        self.test_datasets = self._load_test_datasets()
        self.baseline_metrics = self._load_baseline_metrics()
        
    async def run_comprehensive_evaluation(self) -> EvaluationReport:
        """ì¢…í•© ì„±ëŠ¥ í‰ê°€ ì‹¤í–‰"""
        evaluation_tasks = [
            self._evaluate_parsing_performance(),
            self._evaluate_normalization_performance(),
            self._evaluate_recommendation_quality(),
            self._evaluate_vector_search_performance(),
            self._evaluate_system_throughput()
        ]
        
        results = await asyncio.gather(*evaluation_tasks)
        
        # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        report = EvaluationReport(
            timestamp=datetime.utcnow(),
            parsing_results=results[0],
            normalization_results=results[1],
            recommendation_results=results[2],
            vector_search_results=results[3],
            throughput_results=results[4],
            overall_score=self._calculate_overall_score(results)
        )
        
        # ê¸°ì¤€ì„ ê³¼ ë¹„êµ
        comparison = self._compare_with_baseline(report)
        report.baseline_comparison = comparison
        
        return report
    
    async def _evaluate_parsing_performance(self) -> ParsingEvaluationResult:
        """íŒŒì‹± ì„±ëŠ¥ í‰ê°€"""
        parser = ProductionInputParser(
            llm_client=await self._get_llm_client(),
            config=self.config.parser_config
        )
        
        test_cases = self.test_datasets["parsing"]
        results = []
        
        for test_case in test_cases:
            start_time = time.time()
            
            try:
                parsed_result = await parser.parse_with_context(
                    user_input=test_case["input"],
                    session_context={},
                    conversation_history=[]
                )
                
                parsing_time = time.time() - start_time
                
                # ì •í™•ë„ ê³„ì‚°
                accuracy = self._calculate_parsing_accuracy(
                    parsed_result, test_case["expected"]
                )
                
                results.append(ParsingTestResult(
                    test_id=test_case["id"],
                    input_text=test_case["input"],
                    accuracy=accuracy,
                    parsing_time=parsing_time,
                    confidence=parsed_result.confidence,
                    success=True
                ))
                
            except Exception as e:
                results.append(ParsingTestResult(
                    test_id=test_case["id"],
                    input_text=test_case["input"],
                    accuracy=0.0,
                    parsing_time=time.time() - start_time,
                    confidence=0.0,
                    success=False,
                    error=str(e)
                ))
        
        # í†µê³„ ê³„ì‚°
        successful_results = [r for r in results if r.success]
        
        return ParsingEvaluationResult(
            total_test_cases=len(test_cases),
            successful_cases=len(successful_results),
            average_accuracy=np.mean([r.accuracy for r in successful_results]),
            average_parsing_time=np.mean([r.parsing_time for r in successful_results]),
            accuracy_distribution=self._calculate_accuracy_distribution(successful_results),
            performance_by_scenario=self._analyze_performance_by_scenario(results)
        )
    
    async def _evaluate_recommendation_quality(self) -> RecommendationEvaluationResult:
        """ì¶”ì²œ í’ˆì§ˆ í‰ê°€"""
        recommender = RecommendationEngine()
        test_cases = self.test_datasets["recommendation"]
        
        results = []
        
        for test_case in test_cases:
            start_time = time.time()
            
            recommendations = await recommender.get_recommendations(
                parsed_input=test_case["parsed_input"],
                limit=10
            )
            
            recommendation_time = time.time() - start_time
            
            # ì¶”ì²œ í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            metrics = self._calculate_recommendation_metrics(
                recommendations=recommendations,
                relevant_items=test_case["relevant_items"],
                query_context=test_case["context"]
            )
            
            results.append(RecommendationTestResult(
                test_id=test_case["id"],
                recommendation_count=len(recommendations),
                recommendation_time=recommendation_time,
                precision_at_5=metrics["precision_at_5"],
                recall_at_5=metrics["recall_at_5"],
                diversity_score=metrics["diversity"],
                average_confidence=np.mean([rec.score for rec in recommendations])
            ))
        
        return RecommendationEvaluationResult(
            total_test_cases=len(test_cases),
            average_precision_at_5=np.mean([r.precision_at_5 for r in results]),
            average_recall_at_5=np.mean([r.recall_at_5 for r in results]),
            average_diversity=np.mean([r.diversity_score for r in results]),
            average_recommendation_time=np.mean([r.recommendation_time for r in results]),
            quality_distribution=self._analyze_quality_distribution(results)
        )
    
    def _calculate_parsing_accuracy(self, parsed: ParsedInput, expected: Dict) -> float:
        """íŒŒì‹± ì •í™•ë„ ê³„ì‚°"""
        weights = {
            "location": 0.3,
            "equipment_type": 0.3,
            "status_code": 0.25,
            "priority": 0.15
        }
        
        scores = []
        for field, weight in weights.items():
            parsed_value = getattr(parsed, field, None)
            expected_value = expected.get(field)
            
            if parsed_value == expected_value:
                scores.append(1.0 * weight)
            elif parsed_value and expected_value:
                # ë¶€ë¶„ ë§¤ì¹­ ì ìˆ˜
                similarity = SequenceMatcher(
                    None, 
                    str(parsed_value).lower(), 
                    str(expected_value).lower()
                ).ratio()
                scores.append(similarity * weight)
            else:
                scores.append(0.0)
        
        return sum(scores)
```

## ğŸš€ ë°°í¬ ìë™í™”

### CI/CD íŒŒì´í”„ë¼ì¸

```yaml
# .github/workflows/production-deployment.yml
name: PMark3 Production Deployment

on:
  push:
    branches: [main]
    tags: ['v*']
  pull_request:
    branches: [main]

env:
  AZURE_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }}
  AZURE_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }}
  AZURE_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }}
  AZURE_CLIENT_SECRET: ${{ secrets.AZURE_CLIENT_SECRET }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install -r backend/requirements.txt
        pip install pytest pytest-asyncio
    
    - name: Run unit tests
      run: |
        pytest tests/unit/ -v
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v
    
    - name: Run performance tests
      run: |
        python scripts/performance_test.py

  build:
    needs: test
    runs-on: ubuntu-latest
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2
    
    - name: Login to Azure Container Registry
      uses: azure/docker-login@v1
      with:
        login-server: ${{ secrets.ACR_LOGIN_SERVER }}
        username: ${{ secrets.ACR_USERNAME }}
        password: ${{ secrets.ACR_PASSWORD }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ secrets.ACR_LOGIN_SERVER }}/pmark3
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
    
    - name: Build and push
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy-staging:
    needs: build
    runs-on: ubuntu-latest
    environment: staging
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Azure Login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}
    
    - name: Set up kubectl
      uses: azure/setup-kubectl@v3
    
    - name: Get AKS credentials
      run: |
        az aks get-credentials --resource-group pmark3-staging --name pmark3-aks-staging
    
    - name: Deploy to staging
      run: |
        envsubst < k8s/staging/deployment.yaml | kubectl apply -f -
        kubectl set image deployment/pmark3-api pmark3-api=${{ needs.build.outputs.image-tag }}
        kubectl rollout status deployment/pmark3-api

  deploy-production:
    needs: [build, deploy-staging]
    runs-on: ubuntu-latest
    environment: production
    if: startsWith(github.ref, 'refs/tags/v')
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Deploy to production
      run: |
        # Blue-Green ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
        ./scripts/blue-green-deployment.sh ${{ needs.build.outputs.image-tag }}
```

---

ì´ ê°€ì´ë“œëŠ” PMark3 ì‹œìŠ¤í…œì„ Production í™˜ê²½ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì „í™˜í•˜ê¸° ìœ„í•œ ëª¨ë“  ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ì„ í¬í•¨í•©ë‹ˆë‹¤. ê° ë‹¨ê³„ë³„ë¡œ ì ì§„ì ìœ¼ë¡œ êµ¬í˜„í•˜ë©°, ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ê°œì„ ì„ í†µí•´ ì•ˆì •ì ì¸ Production ì„œë¹„ìŠ¤ë¥¼ êµ¬ì¶•í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. 